% !TeX root = ../main.tex
\chapter{Future Work}\label{chapter:future-work}

In this thesis, three experiments were implemented to show that the smartphone can indeed be used to assist in \ac{VR}. But during the development and after the evaluation, many problems and areas for improvement were discovered. 



\section{UBI Interact}\label{section:fw-ubii}

At the time of writing, a feature of \ac{UBII} called \enquote{muxer} was still in development. This multiplexing feature enables to write interactions that operate on multiple topics. Data of different topics can be combined, evaluated, changed, and then published to different topics. It could be used to handle multiple smartphone connections on the server-side to further abstract the system.

To make use of multitouch displays, which are displays which detect multiple fingers on the display, the smart device client has to be adjusted. It has to be decided if a new format, which supports multiple touch events stored in an array, using multiple topics, or multiple posts to the same topic, makes more sense.

Also, the virtual keyboard experiment could be further abstracted into \ac{UBII} interactions. Theoretically, the client could just send the coordinates to an interaction, and an object with instructions on how to display the virtual keyboard alongside the pressed character or action could be returned.



\section{Experiments}\label{section:fw-experiments}

The tracking problem when holding the phone upside down could be solved. This would be done by implementing a native client to overcome the limitations of the WebAPIs. This would also give access to system buttons and \ac{OS}-layer features. In the current implementation, the fullscreen would sometimes exit, because the border of the screen was touched or a notification appeared. A native application could block these.

The model viewer experiment could also incorporate the touchscreen by allowing to move or change the size of the model.

Further pointing techniques like those from \citeauthor{Argelaguet.2013} can be explored and compared to further improve the usability of the laser pointer~\cite[123]{Argelaguet.2013}.

The evaluation of the virtual keyboard experiment brought many issues to light. 
Values like the sensitivity of the movement detection and selection speed have to be adjusted. Also, as the participants suggested, additional feedback when a key was pressed should be implemented. A cursor should be added to the text input field. Adding support for using multiple fingers at once, would require the changes mentioned in Chapter~\ref{section:fw-ubii}.

Also, in the virtual keyboard experiment, it makes sense to compare further text input methods. Users suggested an implementation like the SwiftKey\footnote{SwiftKey by Microsoft is an application for smartphones which allows to customize the keyboard and introduce swiping based typing. Website: \href{https://www.microsoft.com/swiftkey}{www.microsoft.com/swiftkey}}-keyboard. This keyboard implementation enables to type without lifting the finger. 

Force Touch\footnote{Force Touch (also known as \enquote{3D Touch}) is a touch display technology by Apple. Website: \href{https://developer.apple.com/ios/3d-touch/}{developer.apple.com/ios/3d-touch/}}, which can measure touch pressure intensities, introduces another possibility to implement a keyboard for \ac{VR}. The cursor is shown by slightly touching the screen. Instead of holding the current position, the user would just touch the screen with more intensity to select a key. 

This problem could also be solved by mounting a Leap Motion sensor to the \ac{HMD}. \citeauthor{Afonso.2017} evaluated the use of a Leap Motion sensor mounted to \ac{HMD}, to track the finger movements on a smartphone display. Participants of their user study made fewer errors when using the implementation with a virtual avatar of the hand~\cite[247\psq]{Afonso.2017}. This could be especially useful for the virtual keyboard. A regular smartphone keyboard could then be used since the preview of the touch location does not have to be tracked by the touch display.



\section{Positional Tracking}\label{section:fw-positional-tracking}

All experiments, presented in this thesis, either do not have a virtual representation of the smartphone or a representation where just the rotation is synchronized. The smartphone's position cannot be accurately tracked out of the box. 

When using the Valve Index Base Stations or similar\footnote{The HTC Vive Base Stations or the HTC Vive Pro Base Stations would work as well.}, the Vive Tracker\footnote{The Vive Tracker is a generic tracker, which uses the same technology as the motion controllers. Website: \href{https://www.vive.com/eu/vive-tracker/}{www.vive.com/eu/vive-tracker/}} could be used to track the smartphone. But the system should be generic and not bound to one particular tracking system. Also, the tracker had to be attached to the smartphone, which makes it clumsy.

Since most \acp{HMD} have a camera, a marker could be displayed on the smartphone's screen. This marker could be tracked by the cameras of the \ac{HMD}. But since the positions and view frustums of the cameras vary, this system has to be adjusted to every headset.

The system of \citeauthor{Dias.2018} which is presented in Chapter~\ref{section:dias-2018}, proposes a system, where the front camera of the smartphone is used to track a marker which is stuck to the \ac{HMD}~\cite[4]{Dias.2018}. Additionally, the system from \citeauthor{Afonso.2017} is used to track the hand and fingers with a Leap Motion sensor~\cite[247]{Afonso.2017}.

