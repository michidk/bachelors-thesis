% !TeX root = ../main.tex
\chapter{Related Work}\label{chapter:related-work}


% ModControl â€“ Mobile Phones as a Versatile Interaction Device for Large Screen Applications
%\section{Mobile Phones as a Versatile Interaction Device for Large Screen Applications}\label{section:mobile-phones-interaction-device-large-screen}
\section{Deller et al.}\label{section:deller-2011}
\citeauthor{Deller.2011} propose a modular framework to enable interactions between smartphones and large screen applications. They use a typical client-server architecture with an XML\footnote{XML is a standardized data exchange format, that uses human-readable text.}-based protocol. One difference to the system presented in this thesis in terms of networking is that they differentiate between application clients (the large screen) and interaction clients (the smartphones). They provide different modules for the client app. Some modules offer similar functionality like the ones implemented in the experiments of this thesis: The text module enables the user to enter a text; The accelerometer/magnetometer module sends \ac{IMU} data like acceleration and magnetic field data in the background to the server. They also implemented their framework into an application where users can navigate a map and toggle display settings~\cite{Deller.2011}.


% Phone-based motion control in VR (Klinker)
%\section{Phone-based Motion Control in VR}\label{section:phone-based-motion-control-vr}
\section{Benzina et al.}\label{section:benzina-2011}
A similar problem is solved by \citeauthor{Benzina.2011}. They introduce a system for flying through \acp{VE} in \ac{VR}. A \ac{VR} headset is used, which means the phone display cannot be used to display information because the sight is occluded by the \ac{HMD}. Different methods of controlling the flight movement are presented. They came to the conclusion that the most accurate method for controlling the flight uses an approach, where an airplane metaphor (four \acp{DOF}) is simulated~\cite{Benzina.2011}.


% Mobile Devices for Interaction in Immersive Virtual Environments
%\section{Mobile Devices for Interaction in Immersive Virtual Environments}\label{section:mobile-devices-interaction-ve}
\section{Dias et al.}\label{section:dias-2018}
\citeauthor{Dias.2018} propose a solution, where the smartphone has a visual representation in \ac{VR}. The visual representation also shows information and \ac{UI} on its virtual screen. The camera in the smartphone tracks a marker on the \ac{HMD} to track itself. Because the user interacts with the \ac{UI} using the touch screen of the phone as he would in real life, the fingers have to be tracked. Otherwise, the user would not know where his fingers hit the touch screen, because the sight is occluded by the \ac{HMD}. To solves this, they attached a Leap Motion sensor to the \ac{HMD}, which displays a hand avatar~\cite{Dias.2018}. The setup is shown in Figure~\ref{fig:dias-2018}. 

\begin{figure}[H]
  \centering
  \begin{subfigure}{0.4\textwidth}%
    \includegraphics[width=\textwidth]{figures/related_work/dias_2018_tracking.png}
    \caption{1) The front camera of the smartphone tracks the 2) marker on the \ac{HMD}.}\label{fig:dias-2018-tracking}% chktex 9 % chktex 10
  \end{subfigure}%
  \hspace{0.1\textwidth}%
  \begin{subfigure}{0.4\textwidth}%
    \includegraphics[width=\textwidth]{figures/related_work/dias_2018_virtual_smartphone.png}
    \caption{The virtual smartphone representation in the \ac{VE} while interacting with the \ac{UI}.}\label{fig:dias-2018-virtual-smartphone}
  \end{subfigure}%
  \caption[Tracking setup by Dias et al.]{The tracking system by \citeauthor{Dias.2018}~\protect\cite[4,5]{Dias.2018}.}\label{fig:dias-2018}
\end{figure}


% Design and Implementation of an Immersive Virtual Reality System based on a Smartphone Platform
%\section{Design and Implementation of an Immersive VR System based on a Smartphone Platform}\label{section:design-implementation-vr-system-smartphone-platform}
\section{Steed at al.}\label{section:steed-2013}
The approach by \citeauthor{Steed.2013} also uses a smartphone and a \ac{VR} headset. They implemented a visual representation of the phone. But since they do not have positional tracking for the smartphone, the position is fixed relative to the position of the \ac{HMD}. There are two different possible positions, one in front of the \ac{HMD} and the other one in front of the stomach. The position is switched if a hand raise gesture with the phone in the hand is detected. Gestures and orientation of the smartphone are detected using the data of the \ac{IMU}. On the virtual phone screen, a virtual \ac{UI} is displayed as seen in Figure~\ref{fig:steed-2013-ui}. This \ac{UI} has control elements like buttons, which amongst others can be used to toggle a selection mode. In the selection mode, the phone casts a ray out of the top (similar to a laser pointer) as seen in Figure~\ref{fig:steed-2013-laser-pointer}. The ray direction can be changed by rotating the smartphone. As soon as a \ac{UI} button is pressed, the objects intersecting with the ray are selected~\cite{Steed.2013}.

\begin{figure}[H]
  \centering
  \begin{subfigure}{0.45\textwidth}%
    \centering%
    \includegraphics[height=4cm]{figures/related_work/steed_2013_laser_pointer.png}
    \caption{The virtual device in selection mode.}\label{fig:steed-2013-laser-pointer}% chktex 9 % chktex 10
  \end{subfigure}%
  \hspace{0.1\textwidth}%
  \begin{subfigure}{0.45\textwidth}%
    \centering%
    \includegraphics[height=4cm]{figures/related_work/steed_2013_ui.png}
    \caption{The virtual \ac{UI} and the cursor.}\label{fig:steed-2013-ui}
  \end{subfigure}%
  \caption[Virtual smartphone representation by Steep et al.]{The virtual smartphone representation by \citeauthor{Steed.2013}~\protect\cite[43]{Steed.2013}.}\label{fig:steed-2013}
\end{figure}