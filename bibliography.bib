% This file was created with Citavi 6.3.0.0

@inproceedings{Afonso.2017,
 abstract = {How does the virtual representation of the user's hands influence the performance on a button selection task performed in a tabletbased interaction within an immersive virtual environment? To answer this question, we asked 55 participants to use three conditions: no-hand avatar, realistic avatar and translucent avatar. The participants were faster but made slightly more errors while using the no-avatar condition, and considered easier to perform the task with the translucent avatar.},
 author = {Afonso, Luis and Dias, Paulo and Ferreira, Carlos and Santos, Beatriz Sousa},
 title = {Effect of hand-avatar in a selection task using a tablet as input device in an immersive virtual environment},
 keywords = {button selection task;hand-avatar;immersive virtual environment;input device;mobile device;User study},
 pages = {247--248},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 booktitle = {2017 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2017.7893364}
}


@book{Mould.2010,
 year = {2010},
 title = {Graphics interface 2010: Conference ; GI 2010] ; Ottawa, Ontario, Canada, 31 May - 2 June 2010 ; proceedings},
 address = {Mississauga, Ontario},
 publisher = {{Canadian Information Processing Society (CIPS)}},
 isbn = {978-1-56881-712-5},
 editor = {Mould, David and No{\"e}l, Sylvie}
}


@inproceedings{McGill.2015,
 abstract = {We identify usability challenges facing consumers adopting Virtual Reality (VR) head-mounted displays (HMDs) in a survey of 108 VR HMD users. Users reported significant issues in interacting with, and being aware of their real-world context when using a HMD. Building upon existing work on blending real and virtual environments, we performed three design studies to address these usability concerns. In a typing study, we show that augmenting VR with a view of reality significantly corrected the performance impairment of typing in VR. We then investigated how much reality should be incorporated and when, so as to preserve users' sense of presence in VR. For interaction with objects and peripherals, we found that selectively presenting reality as users engaged with it was optimal in terms of performance and users' sense of presence. Finally, we investigated how this selective, engagement-dependent approach could be applied in social environments, to support the user's awareness of the proximity and presence of others.},
 author = {McGill, Mark and Boland, Daniel and Murray-Smith, Roderick and Brewster, Stephen},
 title = {A Dose of Reality: Overcoming Usability Challenges in VR Head-Mounted Displays},
 keywords = {Augmented Virtuality;Engagement;Virtual reality},
 pages = {2143--2152},
 publisher = {ACM},
 isbn = {9781450331456},
 editor = {Kim, Jinwoo},
 booktitle = {CHI 2015 crossings},
 year = {2015},
 address = {New York, NY},
 doi = {10.1145/2702123.2702382}
}


@incollection{Markussen.2013,
 abstract = {Most text entry methods require users to have physical devices within reach. In many contexts of use, such as around large displays where users need to move freely, device-dependent methods are ill suited. We explore how selection-based text entry methods may be adapted for use in mid-air. Initially, we analyze the design space for text entry in mid-air, focusing on single-character input with one hand. We propose three text entry methods: H4 Mid-Air (an adaptation of a game controller-based method by MacKenzie et al. [21]), MultiTap (a mid-air variant of a mobile phone text entry method), and Projected QWERTY (a mid-air variant of the QWERTY keyboard). After six sessions, participants reached an average of 13.2 words per minute (WPM) with the most successful method, Projected QWERTY. Users rated this method highest on satisfaction and it resulted in the least physical movement.},
 author = {Markussen, Anders and Jakobsen, Mikkel R. and Hornb{\ae}k, Kasper},
 title = {Selection-Based Mid-Air Text Entry on Large Displays},
 keywords = {Huffman coding;large high-resolution displays;mid-air interaction techniques;multitap;text entry},
 pages = {401--418},
 volume = {8117},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-40482-5},
 series = {Lecture Notes in Computer Science / Information Systems and Applications, Incl. Internet/Web, and HCI},
 editor = {Kotze, Paula and Marsden, Gary and Lindgaard, Gitte and Winckler, Marco},
 booktitle = {Human-Computer Interaction -- INTERACT 2013},
 year = {2013},
 address = {Berlin/Heidelberg},
 doi = {10.1007/978-3-642-40483-2{\textunderscore }28}
}


@proceedings{Mark.2017,
 year = {2017},
 title = {Explore, innovate, inspire: CHI 2017 : May 6-11, Denver, CO, USA},
 keywords = {Benutzeroberfl{\"a}che;Mensch-Maschine-Schnittstelle},
 address = {New York, NY},
 publisher = {{Association for Computing Machinery Inc. (ACM)}},
 isbn = {9781450346559},
 editor = {Mark, Gloria and Fussell, Susan and Lampe, Cliff and schraefel, m.c and Hourcade, Juan Pablo and Appert, Caroline and Wigdor, Daniel},
 institution = {CHI and {Association for Computing Machinery} and {ACM CHI} and {ACM CHI Conference on Human Factors in Computing Systems} and {CHI Conference on Human Factors in Computing Systems}},
 doi = {10.1145/3025453}
}


@proceedings{Mandryk.2018,
 year = {2018},
 title = {Engage with CHI: CHI 2018 : proceedings of the 2018 CHI Conference on Human Factors in Computing Systems : April 21 -26, 2018, Montr{\'e}al, QC, Canada},
 keywords = {Benutzeroberfl{\"a}che;Mensch-Maschine-Schnittstelle},
 address = {New York, New York},
 publisher = {{The Association for Computing Machinery}},
 isbn = {9781450356206},
 editor = {Mandryk, Regan and Hancock, Mark},
 institution = {CHI and {Association for Computing Machinery} and {CHI Conference on Human Factors in Computing Systems} and {ACM CHI Conference on Human Factors in Computing Systems} and {Annual CHI Conference on Human Factors in Computing Systems}},
 doi = {10.1145/3173574}
}


@inproceedings{Lipari.2015,
 abstract = {We integrated touch menus into a cohesive smartphone-based VR controller. Smartphone touch surfaces offer new interaction styles and also aid VR interaction when tracking is absent or impreciseor when users have limited arm mobility or fatigue. In Handymenu, a touch surface is split into two areas: one for menu interaction and the other for spatial interactions such as VR object selection, manipulation, navigation, or parameter adjustment. Users in our studies transitioned between the two areas and performed nested, repeated selections. A formal experiment included VR object selection (ray and touch), menu selection (ray and touch), menu layout (pie and grid), as well as touch and visual feedback sizes in some cases (two levels each).},
 author = {Lipari, Nicholas G. and Borst, Christoph W.},
 title = {Handymenu: Integrating menu selection into a multifunction smartphone-based VR controller},
 keywords = {3DTV;Menus;Smartphone;Touch;Virtual reality},
 pages = {129--132},
 publisher = {IEEE},
 isbn = {978-1-4673-6886-5},
 editor = {Lindeman, Rob},
 booktitle = {2015 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2015},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2015.7131737}
}


@inproceedings{Pietroszek.2014,
 abstract = {We introduce and formally evaluate smartcasting: a smartphone-based Ray Casting implementation for 3D environments presented on large, public, autostereoscopic displays. By utilizing a smartphone as an input device, smartcasting enables ``walk up and use'' interaction with large displays, without the need for expensive tracking systems or specialized pointing devices. Through an empirical validation we show that the performance and precision of smartcasting is comparable to a Wiimotebased raycasting implementation, without requiring specialized hardware or high-precision cameras to enable user interaction.},
 author = {Pietroszek, Krzysztof and Kuzminykh, Anastasia and Wallace, James R. and Lank, Edward},
 title = {Smartcasting: A Discount 3D Interaction Technique  for Public Displays},
 keywords = {3D displays;3D environnent;3D interaction;interaction technique;mobile interaction;raycasting},
 pages = {119--128},
 publisher = {ACM},
 isbn = {978-1-4503-0653-9},
 editor = {{Tuck Wah Leong}},
 booktitle = {Proceedings of the 26th Australian Computer-Human Interaction Conference  on Designing Futures - the Future of Design, OZCHI '14, Sydney,  New South Wales, Australia, December 2-5, 2014},
 year = {2014},
 doi = {10.1145/2686612.2686629}
}


@proceedings{Lindeman.2015,
 year = {2015},
 title = {2015 IEEE Symposium on 3D User Interfaces (3DUI): 23 - 24 March 2015, Arles, France},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6886-5},
 editor = {Lindeman, Rob},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE Symposium on 3D User Interfaces} and 3DUI and {IEEE 3DUI Symposium}}
}


@proceedings{Lecuyer.2013,
 year = {2013},
 title = {2013 IEEE Symposium on 3D User Interfaces (3DUI): 16 - 17 March 2013, Orlando, Florida, USA},
 keywords = {Benutzeroberfl{\"a}che;Congresses;Dreidimensionales Bild;Erweiterte Realit{\"a}t;Three-dimensional display systems;User interfaces (Computer systems);Virtual reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6098-2},
 editor = {L{\'e}cuyer, Anatole},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Symposium on 3D User Interfaces} and 3DUI}
}


@book{Latorre.2009,
 abstract = {This book comprises a collection of chapters concerning novel research on Human-Computer Interaction. Chapters correspond to high-quality selected papers from Interaccion 2007, the 8th annual conference on Human-Computer Interaction held in Zaragoza, Spain, as part of the II CEDI, the second edition of the Spanish Conference on Informatics, the biggest and most important in Spain. This book is particularly aimed at researchers, advanced HCI students and practitioners. It includes promising works that range from cutting edge paradigms such as Semantic Web Interfaces, Natural Language Processing and Mobile Interaction to new trends and interface-engineering techniques such as User-Centred Design, Usability, Accessibility, Development Methodologies and Emotional User Interfaces. It also provides a specialized learning reference for advanced lectures, involving topics such as Human Factors, Collaborative User Interfaces and Model-based User Interface Design.},
 year = {2009},
 title = {New Trends on Human-Computer Interaction: Research, Development, New Tools and Methods},
 keywords = {Computer science;Information systems;Mensch-Maschine-Kommunikation;Multimedia systems;Software engineering},
 address = {London},
 publisher = {{Springer-Verlag London}},
 isbn = {978-1-84882-351-8},
 editor = {Latorre, Pedro M. and {Granollers Saltiveri}, Antoni and Mac{\'i}as, Jos{\'e} A.},
 doi = {10.1007/978-1-84882-352-5}
}


@book{Kotze.2013,
 abstract = {Foreword -- Organizing -- Table of Contents -- Long and Short Papers -- 3D Navigation -- Assessing the Impact of Automatic vs. Controlled Rotations on Spatial Transfer with a Joystick and a Walking Interface in VR -- 1 Introduction -- 1.1 Spatial Cognition (cognitive and sensorimotor processes) -- 1.2 Spatial Cognition, Interfaces and Rotational Movements in VR -- 1.3 Spatial Cognition and Spatial Transfer from Virtual to Real Environments -- 2 Method -- 2.1 Setup -- 2.2 Interface Modeling -- 2.3 Procedure -- 3 Results -- 3.1 Learning Phase -- 3.2 Spatial Restitution Tasks -- 3.3 Correlations -- 4 Discussion -- 4.1 Egocentric Tasks -- 4.2 Allocentric Tasks -- 4.3 Spatial Transfer (The wayfinding task) -- 5 Conclusion -- References -- Designing Intuitive Multi-touch 3D Navigation Techniques -- 1 Introduction -- 2 Related Work -- 2.1 Basic Viewpoint Control -- 2.2 Viewpoint Control Facilitation -- 2.3 A Lack of Systematic Approach -- 3 Design Methodology -- 3.1 Identifying Navigation Tasks and Associated Controls -- 3.2 Identifying Input Handles -- 3.3 Choosing the Right t Mappings -- 4 Implementation: The Move and Look Technique -- 4.1 Single-Touch Interaction: -- 4.2 Multi-touch Interaction Switch: RST Classifier -- 4.3 Multi-touch Gestures: Circle Around, Look Around and Scrutinize -- 5 Experiment -- 5.1 Task -- 5.2 Participants -- 5.3 Apparatus, Design and Procedure -- 5.4 Results -- 6 Conclusion and Future Work -- References -- Truly Useful 3D Drawing System for Professional Designer by {\textquotedbl}Life-Sized and Operable{\textquotedbl} Feature and New Interaction -- 1 Introduction -- 2 Related Works and Purpose -- 2.1 Related Works -- 2.2 Purpose of This Paper -- 3 Our Trials for 3D Sketch System -- 3.1 Role of 3D Space -- 3.2 Our 3D System-1: {\textquotedbl}Godzilla{\textquotedbl} -- 3.3 Our 3D System-2: {\textquotedbl}Extended Godzilla{\textquotedbl} -- 3.4 Our 3D System 3: Rich Visual Feedback



3.5 Our 3D System-4: Force Feedback -- 4 Long-Term Evaluation -- 4.1 Short-Term Evaluation Was Promising -- 4.2 Long-Term Evaluation: 3D Space Is Useless -- 4.3 Lack of Indispensability of 3D Space -- 5 New Design Concept -- 5.1 Indispensable Functions of 3D Space -- 5.2 Design Concept {\textquotedbl}Life-Sized and Operable{\textquotedbl} -- 6 New 3D Sketch System with Mixed Reality -- 6.1 New Design Flow -- 6.2 Examples of Design Process -- 6.3 Special User Interaction for the New Design Concept -- 6.4 Current Prototype System -- 6.5 Preliminary Evaluation -- 7 Conclusion -- References -- 3D Technologies - 3D Object Manipulation -- A One-Handed Multi-touch Method for 3D Rotations -- 1 Introduction -- 2 Previous Work -- 3 A New Multi-touch 3D Rotation Technique -- 3.1 Two Multi-touch Rotation Methods -- 3.2 Rotation Only Method -- 3.3 Rotation and Translation Method -- 4 Methodology -- 4.1 Participants -- 4.2 Apparatus -- 4.3 Procedure -- 5 Results -- 5.1 Discussion -- 5.2 Conclusion -- References -- HandsIn3D: Supporting Remote Guidance with Immersive Virtual Environments -- 1 Introduction -- 2 HandsIn3D -- 3 A User Study -- 3.1 Method -- 3.2 Procedure -- 3.3 Results and Discussion -- 4 Concluding Remarks -- References -- MotionBender: A Gesture-Based Interaction Technique for Editing Motion Paths -- 1 Motivation -- 2 Interaction Technique -- 2.1 Overview -- 2.2 Workflow -- 3 Experiment -- 3.1 Methods -- 3.2 Results -- 4 Discussion -- References -- RelicPad: A Hands-On, Mobile Approach to Collaborative Exploration of Virtual Museum Artifacts -- 1 Introduction -- 2 Motivation -- 3 Related Work -- 4 Description of Relic Pad -- 4.1 Overview of RelicPad Features -- 5 First Study - Communication and Collaboration Using Relic Pad -- 5.1 Experiment -- 5.2 Results -- 6 Second Study - Techniques for Manipulating Virtual Museum Artefacts -- 6.1 Experiment -- 6.2 Results -- 7 Discussion



8 Conclusions -- 8.1 Future Work -- References -- Augmented Reality -- Funneling and Saltation Effects for Tactile Interaction with {\textquotedbl}Detached{\textquotedbl} Out of the Body Virtual Objects -- 1 Introduction -- 2 Related Work -- 3 Experiment I: Effects of Funneling with {\textquotedbl}Detached{\textquotedbl} Visual Object -- 3.1 Purpose and Hypothesis -- 3.2 Experimental Design and Set Up -- 3.3 Detailed Procedure -- 3.4 Results -- 4 Experiment II: Effects of Saltation with {\textquotedbl}Detached{\textquotedbl} Visual Object -- 4.1 Purpose and Hypothesis -- 4.2 Experimental Design and Set Up -- 4.3 Detailed Procedure -- 4.4 Results -- 5 Discussion and Conclusion -- References -- Precise Pointing Techniques for Handheld Augmented Reality -- 1 Introduction -- 2 Related Work -- 2.1 Pointing Techniques for Touch-Based Handheld Devices -- 2.2 Pointing Techniques for Handheld AR and Spatially-Aware Interfaces -- 3 Handheld AR Pointing -- 3.1 Design Rationale -- 3.2 Shift and Freeze -- 3.3 Relative Pointing -- 4 Experiments -- 4.1 Experiment 1: User experience -- 4.2 Experiment 2: Performance -- 5 Conclusion and Future Work -- References -- The Unadorned Desk: Exploiting the Physical Space around a Display as an Input Canvas -- 1 Introduction -- 2 Related Work -- 3 Evaluating off-screen Interactions -- 3.1 Conditions Common to Both Experiments -- 3.2 Apparatus, Setup and Participants -- 3.3 Hypotheses -- 4 Study 1 - Placing and Retrieving Content -- 4.1 Tasks and Procedure -- 4.2 Results -- 4.3 Discussion -- 5 Study 2 - Targeting Content -- 5.1 Task and Procedure -- 5.2 Results -- 5.3 Discussion -- 6 General Discussion -- 7 Conclusion and Future Work -- References -- Cognitive Workload -- GSR and Blink Features for Cognitive Load Classification -- 1 Introduction -- 2 Experiment -- 3 Cognitive Load Measurement -- 4 Cognitive Load Classification -- 5 Conclusion -- References



Information Holodeck: Thinking in Technology Ecologies -- 1 Introduction -- 2 A Review of Technology Ecosystems -- 3 A Model of Thinking in DDEs -- 4 The Study -- 5 Findings -- 6 Discussion -- 7 Conclusion -- References -- Managing Personal Information across Multiple Devices: Challenges and Opportunities -- 1 Introduction -- 2 Related Work -- 3 Interview Study -- 3.1 Interview Method -- 3.2 Results -- 4 Design Implications -- 4.1 Organization -- 4.2 Visualization -- 5 Conclusion -- References -- Mobility Matters: Identifying Cognitive Demands That Are Sensitive to Orientation -- 1 Introduction -- 2 Background Literature -- 2.1 Mobility Matters -- 2.2 Does Orientation Matter? -- 2.3 Evidence of the Importance of Presentation Orientation -- 2.4 The Role of Spatial Ability -- 2.5 Summary: Background Literature -- 3 Study: Impact of Orientation of Presentation and Spatial Ability on Construction Task Performance -- 3.1 Experimental Design -- 3.3 Spatial Ability -- 3.6 Procedure -- 3.7 Scoring of the Folded Whales -- 3.8 Summary: Relating the Study to the Hypothesis -- 4 Results -- 4.1 Discussion: Study Results -- 5 Vertical Orientation: Does It Matter? -- 5.1 Discussion: Pilot Study Results -- 6 Conclusions -- References -- Cognitive Workload and Decision Support -- Ambient Timer - Unobtrusively Reminding Users of Upcoming Tasks with Ambient Light -- 1 Introduction -- 2 Related Work -- 3 Ambient Timer -- 4 Methodology -- 4.1 Design -- 4.2 Apparatus -- 4.3 Participants -- 4.4 Procedure -- 5 Results -- 5.1 Objective Measure s -- 5.2 Subjective Measures -- 5.3 Comments and Observations -- 5.4 Discussion -- 6 Conclusion -- References -- Novel Modalities for Bimanual Scrolling on Tablet Devices -- 1 Introduction -- 1.1 Why Bimanual? -- 2 Background -- 2.1 Bimanual Interaction on Touchscreen Devices -- 2.2 Models of Bimanual Action



3 Bimanual Scrolling - Experiment 1 -- 3.1 Input Methods -- 3.2 Interaction Techniques -- 3.3 Participants -- 3.4 Hypothesis -- 3.5 Experimental Design and Procedure -- 4 Results -- 4.1 Overall Results - Interaction Technique and Distance -- 4.2 Detailed Results - Scroll Method/Speed Method and Distance -- 5 Bimanual Scroll ling - Experiment 2 -- 5.1 Interaction Techniq que -- 5.2 Participants -- 5.3 Hypothesis -- 5.4 Experimental Design and Procedure -- 5.5 Results -- 5.6 Discussion -- 6 General Discussion -- 6.1 Dial vs. Touch for Direction Control -- 6.2 Pressure Space -- 7 Conclusions and Future Work -- References -- Public Information System Interface Design Research -- 1 Introduction -- 2 Relevant Research -- 2.1 Resources Model -- 2.2 Universal Usability -- 3 Interaction Model for PIS -- 3.1 PIS Interface Information Structure -- 3.2 PIS Interface Interaction Strategy -- 4 PIS Interaction Design Method -- 4.1 Users' Cognitive Analysis -- 4.2 Interaction Task Analysis -- 4.3 Interface Information Layout -- 4.4 Design Project Framing -- 4.5 Prototype Design -- 4.6 Cognitive Load Evaluation -- 5 Case Study -- 6 Limitations and Recommendations -- References -- Creating Effective 3D Displays -- Comparison of User Performance in Mixed 2D-3D Multi-Display Environments -- 1 Introduction -- 2 Related Work -- 2.1 Multi-Display Environments -- 2.2 Standalone 3D Devices -- 2.3 Visual Comfort in 3D Setups -- 3 Experiment -- 3.1 Conditions -- 3.2 Task -- 3.3 Apparatus -- 3.4 Participants -- 3.5 Procedure -- 3.6 Measures -- 3.7 Results -- 4 Discussion -- 4.1 Interpretation of Results -- 4.2 Implications -- 4.3 Future Work -- 5 Conclusion -- References -- Touching the Void Revisited: Analyses of Touch Behavior on and above Tabletop Surfaces -- 1 Introduction -- 2 Background -- 2.1 Kinematics of Touch -- 2.2 3D Touch for 3D Objects



2.3 2D Touch for 3D Objects},
 year = {2013},
 title = {Human-Computer Interaction -- INTERACT 2013: 14th IFIP TC 13 International Conference, Cape Town, South Africa, September 2-6, 2013, Proceedings, Part I},
 address = {Berlin/Heidelberg},
 volume = {v.8117},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-40482-5},
 series = {Lecture Notes in Computer Science / Information Systems and Applications, Incl. Internet/Web, and HCI},
 editor = {Kotze, Paula and Marsden, Gary and Lindgaard, Gitte and Winckler, Marco},
 doi = {10.1007/978-3-642-40483-2}
}


@phdthesis{Koetsier.2016,
 abstract = {Vampires is a framework that assists in finding the optimal combination of resources to use for execution of a set of independent tasks in a heterogeneous cloud environment. This thesis discusses the decisions made during the development of a web-based user interface for Vampires. This includes a thorough evaluation of client-side JavaScript frameworks, resulting in the choice of AngularJS to use as the basis of the Vampires user interface.},
 author = {Koetsier, Jaap},
 year = {2016},
 title = {Evaluation of JavaScript frame-works for the development of a web-based user interface for Vampires}
}


@proceedings{Kim.2015,
 year = {2015},
 title = {CHI 2015 crossings: CHI 2015 ; proceedings of the 33rd Annual CHI Conference on Human Factors in Computing Systems ; April 18 - 23, 2015, Seoul, Republic of Korea},
 keywords = {Benutzeroberfl{\"a}che;Mensch-Maschine-Schnittstelle},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450331456},
 editor = {Kim, Jinwoo},
 institution = {{Association for Computing Machinery} and {Annual CHI Conference on Human Factors in Computing Systems} and CHI},
 doi = {10.1145/2702123}
}


@inproceedings{Katzakis.2010,
 abstract = {Conventional input devices such as the mouse and keyboard lack in intuitiveness when it comes to 3D manipulation tasks. In this paper, we explore the use of accelerometer and magnetometer equipped mobile phones as 3-DOF controllers in a 3D rotation task. We put the mobile phone up against the established standards, a mouse and a touch pen and compare their performance. Our preliminary evaluation indicates that for this type of task, with only 5 minutes of practice the mobile device is significantly faster than both the mouse and the touch pen.},
 author = {Katzakis, Nicholas and Hori, Masahiro},
 title = {Mobile devices as multi-DOF controllers},
 keywords = {3D input;accelerometer;controller;interaction;magnetometer;rotation;touchscreen},
 pages = {139--140},
 publisher = {IEEE},
 isbn = {978-1-4244-6846-1},
 editor = {Hachet, Martin},
 booktitle = {IEEE Symposium on 3D User Interfaces (3DUI), 2010 ; Waltham, Massachusetts, USA, 20 - 21 March 2010},
 year = {2010},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2010.5444700}
}


@proceedings{Leong.2014,
 abstract = {Annotation},
 year = {2014},
 title = {Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures the Future of Design},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450306539},
 editor = {Leong, Tuck},
 doi = {10.1145/2686612}
}


@proceedings{Rekimoto.2016,
 year = {2016},
 title = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-4189-9},
 series = {UIST '16},
 editor = {Rekimoto, Jun and Igarashi, Takeo}
}


@inproceedings{Rhoton.2002,
 abstract = {Symbolic  input, including text and numeric  input, can be an important user task in applications of virtual environments (VEs). However, very little research has been performed to  support this  task in immersive VEs. This paper presents the results of an empirical  evaluation of four text input techniques for immersive VEs. The techniques include the Pinch Keyboard (a typing emulation technique using pinch gloves), a one- hand chord keyboard, a soft keyboard using a pen {\&} tablet, and speech. The experiment measured both task performance and usability characteristics of the four techniques. Results indicate  that the speech technique is the  fastest, while the pen {\&} tablet keyboard produces  the  fewest  errors. However,  no  single technique exhibited high levels of performance, usability  and user satisfaction.},
 author = {Rhoton, Christopher J. and Bowman, Doug A. and Pinho, Marcio S.},
 title = {Text Input Techniques for Immersive Virtual Environments: an Empirical Comparison},
 pages = {2154--2158},
 publisher = {{SAGE Publications}},
 editor = {{Human Factors} and {Ergonomics Society. Annual meeting}},
 booktitle = {Proceedings of the Human Factors and Ergonomics Society: 46th Annual Meeting, Baltimore, Maryland, September 30 - October 4, 2002 : Bridging Fundamentals {\&} New Opportunities},
 year = {2002},
 address = {Santa Monica, Calif.}
}


@inproceedings{Schops.2014,
 abstract = {We present a direct monocular visual odometry system which runs in real-time on a smartphone. Being a direct method, it tracks and maps on the images themselves instead of extracted features such as keypoints. New images are tracked using direct image alignment, while geometry is represented in the form of a semi-dense depth map. Depth is estimated by filtering over many small-baseline, pixel-wise stereo comparisons. This leads to significantly less outliers and allows to map and use all image regions with sufficient gradient, including edges. We show how a simple world model for AR applications can be derived from semi-dense depth maps, and demonstrate the practical applicability in the context of an AR application in which simulated objects can collide with real geometry.},
 author = {Schops, Thomas and Engel, Jakob and Cremers, Daniel},
 title = {Semi-dense visual odometry for AR on a smartphone},
 pages = {145--150},
 publisher = {IEEE},
 isbn = {978-1-4799-6184-9},
 editor = {Julier, Simon},
 booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2014},
 year = {2014},
 address = {Piscataway, NJ},
 doi = {10.1109/ISMAR.2014.6948420}
}


@misc{You.2019,
 abstract = {Vue.js - The Progressive JavaScript Framework},
 author = {You, Evans},
 year = {2019},
 title = {Vue.js},
 url = {https://vuejs.org/},
 urldate = {17.06.2019}
}


@misc{Yang.2018,
 abstract = {The team continues to work on updates to SteamVR Input, and we've made strides in the overall system and user experience. Recently, we've updated the SteamVR beta with an improved Controller Binding UI - making it simpler to rebind games that haven't implemented the new SteamVR Input API. Now it's even easier for players and developers to create and share new bindings for VR games, for any current and future controllers.},
 author = {Yang, Lawrence},
 year = {2018},
 title = {Guide: Rebinding Games for New Controllers},
 url = {https://steamcommunity.com/games/250820/announcements/detail/1697188096865619876},
 urldate = {05.08.2019}
}


@misc{Weisel.2017,
 abstract = {Cutie Keys is an open source VR drum keyboard written in Unity/C{\#}. The goal of this project is to provide a basic drum keyboard that anyone can use in their own VR project},
 author = {Weisel, Max},
 year = {2017},
 title = {An open-source keyboard to make your own},
 url = {http://www.normalvr.com/blog/an-open-source-keyboard-to-make-your-own/},
 urldate = {26.06.2019}
}


@misc{WeelcoInc.2017,
 author = {{Weelco Inc}},
 year = {2017},
 title = {Unity Asset Store: Keyboard VR Pro},
 url = {https://assetstore.unity.com/packages/tools/input-management/keyboard-vr-pro-83708},
 keywords = {Keyboard VR Pro;Tools/Input Management},
 urldate = {26.06.2019}
}


@article{Watsen.1999,
 abstract = {A fundamental problem hindering the advancement of virtual world development is that of interaction techniques. There is contention between 2D and 3D techniques and uncertainty as to which is appropriate and when. We have developed a simple mechanism to address this problem whereby the user performs tasks appropriate to 2D interfaces with the 3Com PalmPilot handheld computer. The use of a wireless serial connection allows for unencumbered immersion in CAVE-like environments. Our implementation utilizes Bamboo, a dynamically extensible virtual environment toolkit, which enables our design to accommodate new user interfaces on the fly. We are in the early stages of analyzing these tasks and techniques for usability and efficiency. The paper reports techniques that we have implemented, and the specifics of using Bamboo and a PalmPilot for virtual world applications.},
 author = {Watsen, Kent and Darken, Rudolph P. and Capps, Michael V.},
 year = {1999},
 title = {A Handheld Computer as an Interaction Device to a Virtual Environment}
}


@inproceedings{Walker.2017,
 abstract = {The rise of affordable head-mounted displays (HMDs) has raised questions about how to best design user interfaces for this technology. This paper focuses on the use of HMDs for home and office applications that require substantial text input. A physical keyboard is a familiar and effective text input device in normal desktop computing. But without additional camera technology, an HMD occludes all visual feedback about a user's hand position over the keyboard. We describe a system that assists HMD users in typing on a physical keyboard. Our system has a virtual keyboard assistant that provides visual feedback inside the HMD about a user's actions on the physical keyboard. It also provides powerful automatic correction of typing errors by extending a state-of-the-art touchscreen decoder. In a study with 24 participants, we found our virtual keyboard assistant enabled users to type more accurately on a visually-occluded keyboard. We found users wearing an HMD could type at over 40 words-per-minute while obtaining an error rate of less than 5{\%}.},
 author = {Walker, James and Li, Bochao and Vertanen, Keith and Kuhl, Scott},
 title = {Efficient Typing on a Visually Occluded Physical Keyboard},
 keywords = {decoder;head-mounted display;Physical keyboard;text entry},
 pages = {5457--5461},
 publisher = {{Association for Computing Machinery Inc. (ACM)}},
 isbn = {9781450346559},
 editor = {Mark, Gloria and Fussell, Susan and Lampe, Cliff and schraefel, m.c and Hourcade, Juan Pablo and Appert, Caroline and Wigdor, Daniel},
 booktitle = {Explore, innovate, inspire},
 year = {2017},
 address = {New York, NY},
 doi = {10.1145/3025453.3025783}
}


@proceedings{VR.2006,
 year = {2006},
 title = {Virtual Reality Conference, 2006: 25 - 29 March 2006, [Alexandria, VA ; proceedings},
 keywords = {Computersimulation;Virtuelle Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {{IEEE Operations Center}},
 isbn = {1-4244-0224-7},
 institution = {VR and {IEEE Computer Society} and {IEEE Virtual Reality Conference}}
}


@proceedings{UniversitiTeknologiMARA.2011,
 year = {2011},
 title = {International Conference on User Science and Engineering (i-USEr), 2011: Nov. 29 2011 - Dec. 1 2011, Selangor, Malaysia ; proceedings},
 keywords = {Computer software;Congresses;Human factors;Human-computer interaction},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4577-1655-3},
 institution = {{Universiti Teknologi MARA} and {International Conference on User Science and Engineering} and i-USEr}
}


@proceedings{TuckWahLeong.2014,
 year = {2014},
 title = {Proceedings of the 26th Australian Computer-Human Interaction Conference  on Designing Futures - the Future of Design, OZCHI '14, Sydney,  New South Wales, Australia, December 2-5, 2014},
 url = {https://doi.org/10.1145/2686612,  [Add to Citavi project by DOI]},
 publisher = {ACM},
 isbn = {978-1-4503-0653-9},
 editor = {{Tuck Wah Leong}},
 doi = {10.1145/2686612}
}


@proceedings{Thomas.2016,
 year = {2016},
 title = {2016 IEEE Symposium on 3D User Interfaces (3DUI): Greenville, South Carolina, USA, 19-20 March 2016 : proceedings},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1},
 editor = {Thomas, Bruce H. and Lindeman, Rob and Marchal, Maud},
 institution = {{IEEE Symposium on 3D User Interfaces} and {Institute of Electrical and Electronics Engineers} and 3DUI and {IEEE Virtual Reality} and {IEEE VR}}
}


@proceedings{Tan.2011,
 year = {2011},
 title = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450302685},
 series = {ACM Digital Library},
 editor = {Tan, Desney},
 institution = {{Association for Computing Machinery}},
 doi = {10.1145/1979742}
}


@inproceedings{Steed.2013,
 abstract = {With the increasing power of mobile CPUs and GPUs, it is becoming tractable to integrate all the components of an interactive, immersive virtual reality system onto a small mobile device. We present a demonstration of a head-mounted display system integrated onto an iPhone-based platform. In building this demonstration we tackled two main problems. First, how to integrate the userinterface, utilizing the phone itself as an unseen touch interface. Second, how to integrate multiple inertial measuring units to facilitate user interaction. The resulting system indicates the practicality of mobile virtual reality systems based on smartphones.},
 author = {Steed, Anthony and Julier, Simon},
 title = {Design and implementation of an immersive virtual reality system based on a smartphone platform},
 keywords = {3D user interaction;head-mounted display;Mobile virtual reality;selection tasks},
 pages = {43--46},
 publisher = {IEEE},
 isbn = {978-1-4673-6098-2},
 editor = {L{\'e}cuyer, Anatole},
 booktitle = {2013 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2013},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2013.6550195}
}


@inproceedings{Speicher.2018,
 abstract = {In recent years, Virtual Reality (VR) and 3D User Interfaces (3DUI) have seen a drastic increase in popularity, especially in terms of consumer-ready hardware and software. While the technology for input as well as output devices is market ready, only a few solutions for text input exist, and empirical knowledge about performance and user preferences is lacking. In this paper, we study text entry in VR by selecting characters on a virtual keyboard. We discuss the design space for assessing selection-based text entry in VR. Then, we implement six methods that span different parts of the design space and evaluate their performance and user preferences. Our results show that pointing using tracked hand-held controllers outperforms all other methods. Other methods such as head pointing can be viable alternatives depending on available resources. We summarize our findings by formulating guidelines for choosing optimal virtual keyboard text entry methods in VR.},
 author = {Speicher, Marco and Feit, Anna Maria and Ziegler, Pascal and Kr{\"u}ger, Antonio},
 title = {Selection-based Text Entry in Virtual Reality},
 keywords = {mid-air;pointing;task performance.;text entry;user experience;Virtual reality},
 pages = {1--13},
 publisher = {{The Association for Computing Machinery}},
 isbn = {9781450356206},
 editor = {Mandryk, Regan and Hancock, Mark},
 booktitle = {Engage with CHI},
 year = {2018},
 address = {New York, New York},
 doi = {10.1145/3173574.3174221}
}


@inproceedings{Shibata.2016,
 abstract = {Emerging ultra-small wearables like smartwatches pose a design challenge for touch-based text entry. This is due to the ``fat-finger problem,'' wherein users struggle to select elements much smaller than their fingers. To address this challenge, we developed DriftBoard, a panning-based text entry technique where the user types by positioning a movable qwerty keyboard on an interactive area with respect to a fixed cursor point. In this paper, we describe the design and implementation of DriftBoard and report results of a user study on a watch-size touchscreen. The study compared DriftBoard to two ultra-small keyboards, ZoomBoard (tapping-based) and Swipeboard (swiping-based). DriftBoard performed comparably (no significant difference) to ZoomBoard in the major metrics of text entry speed and error rate, and outperformed Swipeboard, which suggests that panning-based typing is a promising input method for text entry on ultra-small touchscreens.},
 author = {Shibata, Tomoki and Afergan, Daniel and Kong, Danielle and Yuksel, Beste F. and MacKenzie, I. Scott and Jacob, Robert J.K.},
 title = {DriftBoard: A Panning-Based Text Entry Technique for Ultra-Small Touchscreens},
 url = {http://doi.acm.org.eaccess.ub.tum.de/10.1145/2984511.2984591,  [Add to Citavi project by DOI]},
 keywords = {driftboard;fat-finger problem;fixed cursor;movable keyboard;soft keyboard;text entry;ultra-small touchscreens},
 pages = {575--582},
 publisher = {ACM},
 isbn = {978-1-4503-4189-9},
 series = {UIST '16},
 editor = {Rekimoto, Jun and Igarashi, Takeo},
 booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/2984511.2984591}
}


@book{Sherman.2003,
 abstract = {Understanding Virtual Reality arrives at a time when the technologies behind virtual reality have advanced to the point that it is possible to develop and deploy meaningful, productive virtual reality applications. The aim of this thorough, accessible exploration is to help you take advantage of this moment, equipping you with the understanding needed to identify and prepare for ways VR can be used in your field, whatever your field may be. 

 By approaching VR as a communications medium, the authors have created a resource that will remain relevant even as the underlying technologies evolve. You get a history of VR, along with a good look at systems currently in use. However, the focus remains squarely on the application of VR and the many issues that arise in the application design and implementation, including hardware requirements, system integration, interaction techniques, and usability. This book also counters both exaggerated claims for VR and the view that would reduce it to entertainment, citing dozens of real-world examples from many different fields and presenting (in a series of appendices) four in-depth application case studies. * Substantive, illuminating coverage designed for technical and business readers and well-suited to the classroom. * Examines VR's constituent technologies, drawn from visualization, representation, graphics, human-computer interaction, and other fields, and explains how they are being united in cohesive VR systems. * Via a companion Web site, provides additional case studies, tutorials, instructional materials, and a link to an open-source VR programming system


Understanding Virtual Reality arrives at a time when the technologies behind virtual reality have advanced to the point that it is possible to develop and deploy meaningful, productive virtual reality applications. The aim of this thorough, accessible exploration is to help you take advantage of this moment, equipping you with the understanding needed to identify and prepare for ways VR can be used in your field, whatever your field may be. By approaching VR as a communications medium, the authors have created a resource that will remain relevant even as the underlying technologies evolve. You get a history of VR, along with a good look at systems currently in use. However, the focus remains squarely on the application of VR and the many issues that arise in the application design and implementation, including hardware requirements, system integration, interaction techniques, and usability. This book also counters both exaggerated claims for VR and the view that would reduce it to entertainment, citing dozens of real-world examples from many different fields and presenting (in a series of appendices) four in-depth application case studies. * Substantive, illuminating coverage designed for technical and business readers and well-suited to the classroom. * Examines VR's constituent technologies, drawn from visualization, representation, graphics, human-computer interaction, and other fields, and explains how they are being united in cohesive VR systems. * Via a companion Web site, provides additional case studies, tutorials, instructional materials, and a link to an open-source VR programming system},
 author = {Sherman, William R. and Craig, Alan B.},
 year = {2003},
 title = {Understanding virtual reality: Interface, application, and design},
 keywords = {COMPUTERS;Electronic books;Human-computer interaction;Human-computer interaction. Virtual reality;Interaction homme-machine (Informatique);Interactive {\&} Multimedia;R{\'e}alit{\'e} virtuelle;Social Aspects;Virtual reality;Virtuelle Realit{\"a}t},
 address = {San Francisco, CA},
 publisher = {{Morgan Kaufmann}},
 isbn = {9781558603530},
 series = {Morgan Kaufmann series in computer graphics and geometric modeling}
}


@inproceedings{Katsuragawa.2016,
 abstract = {We describe the design and evaluation of a freehand, smartwatchbased, mid-air pointing and clicking interaction technique, called Watchpoint. Watchpoint enables a user to point at a target on a nearby large display by moving their arm. It also enables target selection through a wrist rotation gesture. We validate the use of Watchpoint by comparing its performance with two existing techniques: Myopoint, which uses a specialized forearm mounted motion sensor, and a camera-based (Vicon) motion capture system. We show that Watchpoint is statistically comparable in speed and error rate to both systems and, in fact, outperforms in terms of error rate for small (high Fitts's ID) targets. Our work demonstrates that a commodity smartwatch can serve as an effective pointing device in ubiquitous display environments.},
 author = {Katsuragawa, Keiko and Pietroszek, Krzysztof and Wallace, James R. and Lank, Edward},
 title = {Watchpoint: Freehand Pointing with a Smartwatch in a Ubiquitous Display Environment},
 keywords = {Large displays;pointing;smartwatch;wearable},
 pages = {128--135},
 publisher = {ACM},
 isbn = {978-1-4503-4131-8},
 editor = {Buono, Paolo and Lanzilotti, Rosa and Matera, Maristella},
 booktitle = {AVI '16: Proceedings of the International Working Conference on Advanced Visual Interfaces},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/2909132.2909263}
}


@phdthesis{Kamm.2018,
 abstract = {One of the primary goals in the mixed reality continuum is it to create interaction techniques that feel as natural and immersive as possible. However, it is unclear to which degree this naturalism can be achieved and whether it is always a desirable goal.In the past, many selection techniques have prioritized high accuracy and usability but on modern hardware devices, like the leap motion controller, more natural and bare-handbased interactions have been designed. What is still lacking is a test of these techniques outside of desktop-based environments.With this thesis, we contribute to the discussion around new selection methods in three ways. (1) We introduce the HeadRay, a relative pointing technique for VR, which does not depend on the precise position of the hand, and which therefore is very well suited for use with gesture-based input devices like the Myo armband. We show in a user survey that our technique is a well-working selection method that substantially increases immersion. At the same time, it still suffers from inconsistency in the gesture recognition of the Myo that hamper the usability. (2) We show that it is possible to design test scenarios that fully utilize the virtual space around the user and still allow to make measurements that highly correlate with objective performance and subjective ratings by the user. (3) We demonstrate that the Depth Marker proposed by Grossman et al. is a good way to handle occlusion in VR.},
 author = {Kamm, Clemens},
 year = {2018},
 title = {Precision of Pointing with Myo: A Comparison of Controller- and Gesture-Based Selection in Virtual Reality},
 url = {https://drive.google.com/file/d/10LiDxn3zx_S8gCAcJN7eC8iK1a8aKflf/view},
 address = {Munich},
 urldate = {20.06.2019},
 school = {{Technische Universit{\"a}t M{\"u}nchen}},
 type = {Master's Thesis}
}


@proceedings{Julier.2014,
 year = {2014},
 title = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2014: 10 - 12 Sept. 2014, Munich, Germany},
 keywords = {Erweiterte Realit{\"a}t;Mixed Reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4799-6184-9},
 editor = {Julier, Simon},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE International Symposium on Mixed and Augmented Reality} and ISMAR and {IEEE International Symposium on Mixed and Augmented Reality - Science {\&} Technology}}
}


@incollection{Jota.2010,
 abstract = {Ray-pointing  techniques  are  often  advocated  as  a  way  for  people  to interact with very large displays from several meters away. We are  interested  in  two  factors  that  can  affect  ray  pointing:  the  par-ticular technique's control type, and parallax.  Consequently,  we  tested  four  ray  pointing  variants  on  a  wall  display  that  covers  a  large  part  of  the  user's  field  of  view.  Tasks  included horizontal and vertical targeting, and tracing. Our results show  that  (a)  techniques  based  on  'rotational  control'  perform  better for targeting tasks, and (b) techniques with low parallax are best  for  tracing  tasks.  We  also  show  that  a  Fitts's  law  analysis  based  on  angles  (as  opposed  to  linear  distances)  better  approx-imates people's ray pointing performance.},
 author = {Jota, Ricardo and Nacenta, Miguel A. and Jorge, Joaquim A. and Carpendale, Sheelagh and Greenberg, Saul},
 title = {A comparison of ray pointing techniques for very large displays},
 url = {http://dl.acm.org.eaccess.ub.tum.de/ft_gateway.cfm?id=1839261&type=pdf},
 keywords = {distant pointing;image-plane;index of difficulty;ISO 9241;Large displays;parallax;ray pointing;targeting;tracing},
 urldate = {20.06.2019},
 pages = {269--276},
 publisher = {{Canadian Information Processing Society (CIPS)}},
 isbn = {978-1-56881-712-5},
 editor = {Mould, David and No{\"e}l, Sylvie},
 booktitle = {Graphics interface 2010},
 year = {2010},
 address = {Mississauga, Ontario}
}


@misc{Denoyel.2016,
 abstract = {Discover how you can explore virtual reality on Sketchfab with our new apps for Oculus, HTC Vive, Gear VR and Cardboard as well as on the web with WebVR.},
 author = {Denoyel, Alban},
 year = {2016},
 title = {Virtual Reality evolved: Sketchfab VR apps and WebVR support},
 url = {https://sketchfab.com/blogs/community/announcing-sketchfab-vr-apps-webvr-support/},
 urldate = {23.06.2019},
 originalyear = {16.05.2016}
}


@incollection{Deller.2011,
 abstract = {Large, public displays are increasingly popular in today's society. For the most part, however, these displays are purely used for information or multimedia presentation, without the possibility of interaction for viewers. On the other hand, personal mobile devices are becoming more and more ubiquitous. Though there are efforts to combine large screens with mobile devices, the approaches are mostly focused on mobiles as control devices, or they are fitted to specific applications. In this paper, we present the ModControl framework, a configurable, modular communication structure that enables large screen applications to connect with personal mobile devices and request a set of configurable modules, utilizing the device as a personalized mobile interface. The main application can easily make use of the highly sophisticated interaction features provided by modern mobile phones. This facilitates new, interactive appealing visualizations that can be actively controlled with an intuitive, unified interface by single or multiple users.},
 author = {Deller, Matthias and Ebert, Achim},
 title = {ModControl -- Mobile Phones as a Versatile Interaction Device for Large Screen Applications},
 keywords = {Distributed interfaces;Input devices and strategies;Interaction framework;User-Centered Design},
 pages = {289--296},
 volume = {6947},
 publisher = {Springer},
 isbn = {978-3-642-23770-6},
 series = {Lecture Notes in Computer Science},
 editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
 booktitle = {Human-computer interaction - INTERACT 2011},
 year = {2011},
 address = {Berlin},
 doi = {10.1007/978-3-642-23771-3{\textunderscore }22}
}


@inproceedings{Choi.2018,
 abstract = {CLAW is a handheld virtual reality controller that augments the typical controller functionality with force feedback and actuated movement to the index finger. Our controller enables three distinct interactions (grasping virtual object, touching virtual surfaces, and triggering) and changes its corresponding haptic rendering by sensing the differences in the user's grasp. A servo motor coupled with a force sensor renders controllable forces to the index finger during grasping and touching. Using position tracking, a voice coil actuator at the index fingertip generates vibrations for various textures synchronized with finger movement. CLAW also supports a haptic force feedback in the trigger mode when the user holds a gun. We describe the design considerations for CLAW and evaluate its performance through two user studies. The first study obtained qualitative user feedback on the naturalness, effectiveness, and comfort when using the device. The second study investigated the ease of the transition between grasping and touching when using our device.},
 author = {Choi, Inrak and Ofek, Eyal and Benko, Hrvoje and Sinclair, Mike and Holz, Christian},
 title = {CLAW},
 pages = {1--13},
 publisher = {{The Association for Computing Machinery}},
 isbn = {9781450356206},
 editor = {Mandryk, Regan and Hancock, Mark},
 booktitle = {Engage with CHI},
 year = {2018},
 address = {New York, New York},
 doi = {10.1145/3173574.3174228}
}


@proceedings{Catarci.2018,
 year = {2018},
 title = {AVI '18: Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-5616-9},
 editor = {Catarci, Tiziana and Leotta, Francesco and Marrella, Andrea and Mecella, Massimo}
}


@book{Campos.2011,
 year = {2011},
 title = {Human-computer interaction - INTERACT 2011: 13th IFIP TC 13 international conference, Lisbon, Portugal, September 2011; Proceedings, Part II},
 keywords = {Artificial intelligence;Computer science;Education;Information systems;Software engineering},
 address = {Berlin},
 volume = {6947},
 publisher = {Springer},
 isbn = {978-3-642-23770-6},
 series = {Lecture Notes in Computer Science},
 editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
 doi = {10.1007/978-3-642-23771-3}
}


@misc{Cabello.2019,
 abstract = {JavaScript 3D library. Contribute to mrdoob/three.js development by creating an account on GitHub.},
 author = {Cabello, Ricardo},
 year = {2019},
 title = {Three.js: JavaScript 3D library},
 url = {https://github.com/mrdoob/three.js/},
 urldate = {17.06.2019}
}


@proceedings{Buono.2016,
 year = {2016},
 title = {AVI '16: Proceedings of the International Working Conference on Advanced Visual Interfaces},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-4131-8},
 editor = {Buono, Paolo and Lanzilotti, Rosa and Matera, Maristella}
}


@article{Brooke.1996,
 abstract = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for ``quick and dirty'' methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.},
 author = {Brooke, John},
 year = {1996},
 title = {SUS - A quick and dirty usability scale},
 url = {https://ci.nii.ac.jp/naid/10018890922/en/},
 journal = {Usability Evaluation in Industry}
}


@article{Bowman.2012,
 abstract = {3D UIs are uniquely able to achieve superior interaction fidelity, and this naturalism can be a huge advantage.},
 author = {Bowman, Doug A. and McMahan, Ryan P. and Ragan, Eric D.},
 year = {2012},
 title = {Questioning naturalism in 3D user interfaces},
 pages = {78},
 volume = {55},
 number = {9},
 issn = {00010782},
 journal = {Communications of the ACM},
 doi = {10.1145/2330667.2330687}
}


@incollection{Berge.,
 abstract = {3D Virtual Environments (3DVE) come up as a good solution to transmit knowledge in a museum exhibit. In such contexts, providing easy to learn and to use interaction techniques which facilitate the handling inside a 3DVE is crucial to maximize the knowledge transfer. We took the opportunity to design and implement a software platform for explaining the behavior of the Telescope Bernard-Lyot to museum visitors on top of the Pic du Midi. Beyond the popularization of a complex scientific equipment, this platform constitutes an open software environment to easily plug different 3D interaction techniques. Recently, popular use of a smartphones as personal handled computer lets us envision the use of a mobile device as an interaction support with these 3DVE. Accordingly, we design and propose how to use the smartphone as a tangible object to navigate inside a 3DVE. In order to prove the interest in the use of smartphones, we compare our solution with available solutions: keyboard-mouse and 3D mouse. User experiments confirmed our hypothesis and particularly emphasizes that visitors find our solution more attractive and stimulating. Finally, we illustrate the benefits of our software framework by plugging alternative interaction techniques for supporting selection and manipulation task in 3D.},
 author = {Berg{\'e}, Louis-Pierre and Perelman, Gary and Hamelin, Adrien and Raynal, Mathieu and Sanza, C{\'e}dric and Houry-Panchetti, Minica and Cabanac, R{\'e}mi and Dubois, Emmanuel},
 title = {Smartphone Based 3D Navigation Techniques in an Astronomical Observatory Context: Implementation and Evaluation in a Software Platform},
 url = {https://hal.archives-ouvertes.fr/hal-01387801/document},
 keywords = {3D environment;3D navigation;Experiment;Interaction with smartphone;Interactive visualization;Museum exhibit;Software platform}
}


@inproceedings{Benzina.2011,
 abstract = {We introduce a one-handed travel technique for virtualenvironments (VE), we call Phone-Based MotionControl. The travel technique uses a mobile phone withintegrated sensors as a 3D spatial input device. Webenefit from the touch capability to change theviewpoint translation in the VE, while the orientation ofthe viewpoint in the VE is controlled by the built-insensors. The travel interaction clearly distinguishesbetween translation (touch based translation control)and rotation (steer based rotation control), puttingeach set of degrees of freedom to a separateinteraction technique.This work examines how many degrees of freedom areneeded to perform the travel task as easy as possible.It also investigates different mapping functionsbetween the user's actions and the viewpoint reactionsin the VR. For that purpose, four metaphors aredeveloped for the steer based rotation controltechnique. The results of the user study indicate thetrend that 4 DOF metaphors perform best, and that theusage of a mobile roll to control the viewpoint is thedesired mapping.},
 author = {Benzina, Amal and Toennis, Marcus and Klinker, Gudrun and Ashry, Mohamed},
 title = {Phone-based motion control in VR},
 keywords = {degree of freedom;interaction;Navigation;Travel;User study},
 pages = {1519},
 publisher = {ACM},
 isbn = {9781450302685},
 series = {ACM Digital Library},
 editor = {Tan, Desney},
 booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 year = {2011},
 address = {New York, NY},
 doi = {10.1145/1979742.1979801}
}


@inproceedings{Bauer.2011,
 abstract = {Due to their size, large high-resolution screens have become popular display devices used in collaborative scenarios. However, traditional interaction methods based on combinations of computer mice and keyboards often do not scale to the number of users or the size of the display. Modern smart phones featuring multi-modal input/output by means of built-in cameras, acceleration sensors, internet capability, touch screens and considerable memory offer a way to address these issues. In the last couple of years they have become common everyday life gadgets. In this paper we conduct an extensive user study comparing the experience of test candidates when using traditional input devices and metaphors with the one when using new smart phone based techniques, like multi-modal drag and tilt. Candidates were asked to complete various 2D and 3D interaction tasks relevant for most applications on a large, monitor-based, high-resolution tiled wall system. Our study evaluates both user performance and satisfaction, identifying strengths and weaknesses of the researched interaction methods in specific tasks. By breaking these tasks down into a well-defined set of subtasks the results of each task are comparable to each other and can be classified by subtask and use case. Results reveal a superior performance of users in certain tasks when using the new interaction techniques. Even first-time users were able to complete a task faster with the smart phone than with traditional devices.},
 author = {Bauer, Jens and Thelen, Sebastian and Ebert, Achim},
 title = {Using smart phones for large-display interaction},
 keywords = {Collaboration;evaluation methods/usability evaluation;interaction with small or large displays;mobility/mobile accessibility/mobile devices;multi-modal interfaces},
 pages = {42--47},
 publisher = {IEEE},
 isbn = {978-1-4577-1655-3},
 booktitle = {International Conference on User Science and Engineering (i-USEr), 2011},
 year = {2011},
 address = {Piscataway, NJ},
 doi = {10.1109/iUSEr.2011.6150533}
}


@article{Bangor.2009,
 abstract = {The System Usability Scale (SUS) is an inexpensive, yet effective tool for assessing the usability of a product, including Web sites, cell phones, interactive voice response systems, TV applications, and more. It provides an easy-to-understand score from 0 (negative) to 100 (positive). While a 100-point scale is intuitive in many respects and allows for relative judgments, information describing how the numeric score translates into an absolute judgment of usability is not known. To help answer that question, a seven-point adjective-anchored Likert scale was added as an eleventh question to nearly 1,000 SUS surveys. Results show that the Likert scale scores correlate extremely well with the SUS scores (r=0.822). The addition of the adjective rating scale to the SUS may help practitioners interpret individual SUS scores and aid in explaining the results to non-human factors professionals.},
 author = {Bangor, Aaron and Kortum, Philip and Miller, James},
 year = {2009},
 title = {Determining What Individual SUS Scores Mean: Adding an Adjective Rating Scale},
 url = {http://dl.acm.org.eaccess.ub.tum.de/citation.cfm?id=2835587.2835589},
 keywords = {surveys;SUS;system usability scale;usability;user satisfaction},
 pages = {114--123},
 volume = {4},
 number = {3},
 issn = {1931-3357},
 journal = {J. Usability Studies}
}


@article{Ballagas.2006,
 author = {Ballagas, R. and Borchers, J. and Rohs, M. and Sheridan, J. G.},
 year = {2006},
 title = {The Smart Phone: A Ubiquitous Input Device},
 pages = {70--77},
 volume = {5},
 number = {1},
 issn = {1536-1268},
 journal = {IEEE Pervasive Computing},
 doi = {10.1109/MPRV.2006.18}
}


@article{Argelaguet.2013,
 abstract = {Computer graphics applications controlled through natural gestures are gaining increasing popularity these days due to recent developments in low-cost tracking systems and gesture recognition technologies. Although interaction techniques through natural gestures have already demonstrated their benefits in manipulation, navigation and avatar-control tasks, effective selection with pointing gestures remains an open problem. In this paper we survey the state-of-the-art in 3D object selection techniques. We review important findings in human control models, analyze major factors influencing selection performance, and classify existing techniques according to a number of criteria. Unlike other components of the application's user interface, pointing techniques need a close coupling with the rendering pipeline, introducing new elements to be drawn, and potentially modifying the object layout and the way the scene is rendered. Conversely, selection performance is affected by rendering issues such as visual feedback, depth perception, and occlusion management. We thus review existing literature paying special attention to those aspects in the boundary between computer graphics and human--computer interaction.},
 author = {Argelaguet, Ferran and Andujar, Carlos},
 year = {2013},
 title = {A survey of 3D object selection techniques for virtual environments},
 pages = {121--136},
 volume = {37},
 number = {3},
 issn = {00978493},
 journal = {Computers {\&} Graphics},
 doi = {10.1016/j.cag.2012.12.003}
}


@misc{DevicesandSensorsWorkingGroup.2019,
 abstract = {This specification defines several new DOM events that provide information about the physical orientation and motion of a hosting device.},
 author = {{Devices and Sensors Working Group}},
 editor = {{Devices and Sensors Working Group}},
 year = {2019},
 title = {DeviceOrientation Event Specification: W3C Working Draft, 16 April 2019},
 urldate = {17.06.2019}
}


@article{Yu.2018,
 abstract = {We present PizzaText, a circular keyboard layout technique for text entry in virtual reality (VR) environments that uses the dual thumbsticks of a hand-held game controller. Text entry is a common activity in VR environments but remains challenging with existing techniques and keyboard layouts that is largely based on QWERTY. Our technique makes text entry simple, easy, and efficient, even for novice users. The technique uses a hand-held controller because it is still an important input device for users to interact with VR environments. To allow rapid search of characters, PizzaText divides a circle into slices and each slice contains 4 characters. To enable fast selection, the user uses the right thumbstick for traversing the slices, and the left thumbstick for choosing the letters. The design of PizzaText is based on three criteria: efficiency, learnability, and ease-of-use. In our first study, six potential layouts are considered and evaluated. The results lead to a design with 7 slices and 4 letters per slice. The final design is evaluated in a five-day study with 10 participants. The results show that novice users can achieve an average of 8.59 Words per Minute (WPM), while expert users are able to reach 15.85 WPM, with just two hours of training.},
 author = {Yu, Difeng and Fan, Kaixuan and Zhang, Heng and Monteiro, Diego and Xu, Wenge and Liang, Hai-Ning},
 year = {2018},
 title = {PizzaText: Text Entry for Virtual Reality Systems Using Dual Thumbsticks},
 keywords = {circular keyboard layout;dual-joystick input;game controller;selection keyboard;text entry;Virtual reality},
 pages = {2927--2935},
 volume = {24},
 number = {11},
 journal = {IEEE transactions on visualization and computer graphics},
 doi = {10.1109/TVCG.2018.2868581}
}


@inproceedings{Dias.2018,
 abstract = {Gamepads and 3D controllers are the main controllers used in most Virtual Environments. Despite being simple to use, these input devices have a number of limitations as fixed layout and difficulty to remember the mapping between buttons and functions. Mobile devices present interesting characteristics that might be valuable in immersive environments: more flexible interfaces, touchscreen combined with onboard sensors that allow new interaction and easy acceptance since these devices are used daily by most users. The work described in this article proposes a solution that uses mobile devices to interact with Immersive Virtual Environments for selection and navigation tasks. The proposed solution uses the mobile device camera to track the Head-Mounted-Display position and present a virtual representation of the mobile device screen; it was tested using an Immersive Virtual Museum as use case. Based on this prototype, a study was performed to compare controller based and mobile based interaction for navigation and selection showing that using mobile devices is viable in this context and offers interesting interaction opportunities.},
 author = {Dias, Paulo and Afonso, Luis and Eliseu, S{\'e}rgio and Santos, Beatriz Sousa},
 title = {Mobile devices for interaction in immersive virtual environments},
 url = {http://dl.acm.org.eaccess.ub.tum.de/ft_gateway.cfm?id=3206526&type=pdf},
 keywords = {3D interaction;immersive virtual reality;mobile devices},
 urldate = {24.06.2019},
 publisher = {ACM},
 isbn = {978-1-4503-5616-9},
 editor = {Catarci, Tiziana and Leotta, Francesco and Marrella, Andrea and Mecella, Massimo},
 booktitle = {AVI '18: Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
 year = {2018},
 address = {New York, NY, USA},
 doi = {10.1145/3206505.3206526}
}


@book{ECMAInternational.2018,
 abstract = {This Ecma Standard defines the ECMAScript 2018 Language. It is the ninth edition of the ECMAScript Language Specification. Since publication of the first edition in 1997, ECMAScript has grown to be one of the world's most widely used general-purpose programming languages. It is best known as the language embedded in web browsers but has also been widely adopted for server and embedded applications.},
 author = {{ECMA International}},
 year = {2018},
 title = {Standard ECMA-262: ECMAScript 2018 Language Specification},
 url = {https://www.ecma-international.org/publications/standards/Ecma-262.htm},
 keywords = {javascript},
 urldate = {24.06.2019},
 edition = {9},
 originalyear = {1999}
}


@article{JiYoungOh.2002,
 abstract = {Single Display Groupware (SDG) is a research area that focuses on providing collaborative computing environments. Traditionally, most hardware platforms for SDG support only one person interacting at any given time, which limits collaboration. In this paper, we present laser pointers as input devices that can provide concurrent input streams ideally required to the SDG environment. First, we discuss several issues related to utilization of laser

pointers and present the new concept of computer controlled laser pointers. Then we briefly present a performance evaluation of laser pointers as input devices and a baseline comparison with the mouse according to the ISO 9241-9 standard. Finally, we describe a new system that uses multiple computer controlled laser pointers as interaction devices for one or more displays. Several alternatives for distinguishing between different laser pointers are presented, and an implementation of one of them is demonstrated with SDG applications.},
 author = {{Ji-Young Oh}, Wolfgang Stuerzlinger},
 year = {2002},
 title = {Laser Pointers as Collaborative Pointing Devices},
 url = {https://ci.nii.ac.jp/naid/10017144609/en/},
 journal = {Proc. GI2002-Graphics Interface, Calgary, Canada, May}
}


@inproceedings{Jeon.2007,
 abstract = {Today's mobile phones have not only become the most representative device in the new ubiquitous computing era but also dramatically improved in terms of their multi-modal sensing and display capabilities. This advance makes the mobile phone an ideal candidate for a more natural interaction device in ubiquitous computing environment. This paper proposes techniques which use camera-equipped mobile phones for interacting with 2D and 3D applications on a tabletop display environment. The camera acts as the main sensor for a gesture-based interaction. Using the mobile phone with an interactive touch screen allows the use of techniques that move beyond single hand/finger input to improve task performance. The interaction performances of the proposed techniques and design guidelines are also described in this paper.},
 author = {Jeon, Seokhee and Kim, Gerard J. and Billinghurst, Mark},
 title = {Interacting with a Tabletop Display Using a Camera Equipped Mobile Phone},
 url = {https://link-springer-com.eaccess.ub.tum.de/content/pdf/10.1007%2F978-3-540-73107-8_38.pdf},
 pages = {336--343},
 publisher = {{Springer, Berlin, Heidelberg}},
 isbn = {978-3-540-73107-8},
 editor = {Jacko, Julie A.},
 booktitle = {Human-Computer Interaction. Interaction Platforms and Techniques: 12th International Conference, HCI International 2007, Beijing, China, July 22-27, 2007, Proceedings, Part II},
 year = {2007},
 doi = {10.1007/978-3-540-73107-8{\textunderscore }38}
}


@proceedings{Jacko.2007,
 year = {2007},
 title = {Human-Computer Interaction. Interaction Platforms and Techniques: 12th International Conference, HCI International 2007, Beijing, China, July 22-27, 2007, Proceedings, Part II},
 publisher = {{Springer, Berlin, Heidelberg}},
 isbn = {978-3-540-73107-8},
 editor = {Jacko, Julie A.}
}


@proceedings{IEEESymposiumon3DUserInterfaces.2017,
 year = {2017},
 title = {2017 IEEE Symposium on 3D User Interfaces (3DUI): Proceedings : March 18-19, 2017, Los Angeles, CA, USA},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 institution = {{IEEE Symposium on 3D User Interfaces} and {IEEE Computer Society} and 3DUI}
}


@proceedings{HumanFactors.2002,
 year = {2002},
 title = {Proceedings of the Human Factors and Ergonomics Society: 46th Annual Meeting, Baltimore, Maryland, September 30 - October 4, 2002 : Bridging Fundamentals {\&} New Opportunities},
 url = {https://books.google.de/books?id=GuZfnQAACAAJ},
 address = {Santa Monica, Calif.},
 publisher = {{SAGE Publications}},
 editor = {{Human Factors} and {Ergonomics Society. Annual meeting}},
 institution = {{The Society}}
}


@proceedings{Hepper.2012,
 year = {2012},
 title = {2012 IEEE International Conference on Consumer Electronics - Berlin (ICCE-Berlin 2012): Berlin, Germany, 3 - 5 September 2012},
 keywords = {ilmpub},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-1547-0},
 editor = {Hepper, Dietmar},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Consumer Electronics Society} and {IEEE International Conference on Consumer Electronics - Berlin} and {IEEE ICCE-Berlin}}
}


@proceedings{Hachet.2010,
 year = {2010},
 title = {IEEE Symposium on 3D User Interfaces (3DUI), 2010 ; Waltham, Massachusetts, USA, 20 - 21 March 2010},
 keywords = {Benutzeroberfl{\"a}che;Congresses;Dreidimensionales Bild;Erweiterte Realit{\"a}t;Three-dimensional display systems;User interfaces (Computer systems);Virtual reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4244-6846-1},
 editor = {Hachet, Martin},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Symposium on 3D User Interfaces} and 3DUI}
}


@inproceedings{Grandi.2016,
 abstract = {We present a 3D user interface for collaborative manipulation ofthree-dimensional objects in virtual environments. It maps inertial sensors, touch screen and physical buttons of a mobile phone into well-known gestures to alter the position, rotation and scale of virtualobjects. As these transformations require the control of multiple degrees of freedom (DOFs), collaboration is proposed as a solution to coordinate the modification of each and all the available DOFs. Users are free to decide their own manipulation roles. All virtual elements are displayed in a single shared screen, which is handy to aggregate multiple users in the same physical space.},
 author = {Grandi, Jeronimo G. and Berndt, Iago and Debarba, Henrique G. and Nedel, Luciana and Maciel, Anderson},
 title = {Collaborative 3D manipulation using mobile phones},
 keywords = {3D interaction;Collaboration;controller;mobile devices},
 pages = {279--280},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1},
 editor = {Thomas, Bruce H. and Lindeman, Rob and Marchal, Maud},
 booktitle = {2016 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2016},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2016.7460079}
}


@inproceedings{Graf.2012,
 abstract = {This paper presents a concept for the use of a smartphone as a handheld input device for the interaction with 3D visualizations and presentations. Applications are mainly in the area of exhibitions and museums to enable visitors to interact with certain exhibits. Moreover applications for business presentations or computer games are possible. The usability of motion and position sensors in modern smartphones for the purpose of 3D navigation is examined. An algorithm to prevent from position drifting on short distances is developed. Finally a demo application presents the navigation concept and the ability for multiple users to simultaneously interact with the same visualization.},
 author = {Graf, Henning and Jung, Klaus},
 title = {The smartphone as a 3D input device},
 keywords = {3D navigation;human-device interaction;mobile devices},
 pages = {254--257},
 publisher = {IEEE},
 isbn = {978-1-4673-1547-0},
 editor = {Hepper, Dietmar},
 booktitle = {2012 IEEE International Conference on Consumer Electronics - Berlin (ICCE-Berlin 2012)},
 year = {2012},
 address = {Piscataway, NJ},
 doi = {10.1109/ICCE-Berlin.2012.6336487}
}


@misc{GoogleLLC.2019,
 abstract = {When you are finished with your 3D creation, you can save your Tilt Brush sketch to your computer, or share it to Poly. You can also take snapshot, GIFs or videos of your art to send to friends or post online.},
 author = {{Google LLC}},
 year = {2019},
 title = {Tilt Brush Help: Saving and sharing your Tilt Brush sketches},
 url = {https://support.google.com/tiltbrush/answer/6389651?hl=en},
 urldate = {26.06.2019}
}


@misc{GoogleLLC.2019b,
 abstract = {Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.},
 author = {{Google LLC}},
 year = {2019},
 title = {Protocol Buffers},
 url = {https://developers.google.com/protocol-buffers/},
 urldate = {19.06.2019}
}


@incollection{Gonzalez.2009,
 abstract = {This chapter describes six different text input techniques and their evaluation. Two techniques stand out from the others. First, a mobile phone keyboard turn out to be a valuable option, offering high typing speed and low typing errors. Second, handwritten character recognition did not perform as good as expected. All findings from this and previous experiments are collected in a proposed guidance tool which will promote their application in future projects.},
 author = {Gonz{\'a}lez, Gabriel and Molina, Jos{\'e} P. and Garc{\'i}a, Arturo S. and Mart{\'i}nez, Diego and Gonz{\'a}lez, Pascual},
 title = {Evaluation of Text Input Techniques in Immersive Virtual Environments},
 pages = {109--118},
 publisher = {{Springer-Verlag London}},
 isbn = {978-1-84882-351-8},
 editor = {Latorre, Pedro M. and {Granollers Saltiveri}, Antoni and Mac{\'i}as, Jos{\'e} A.},
 booktitle = {New Trends on Human-Computer Interaction},
 year = {2009},
 address = {London},
 doi = {10.1007/978-1-84882-352-5{\textunderscore }11}
}


@inproceedings{Frees.2006,
 abstract = {We present a novel text input interface for immersive virtual environments called CTD, or {\textquotedbl}Connect the Dots{\textquotedbl}. The CTD interface is a small virtual panel containing a grid of dots the user can connect to form alphanumeric characters using a hand held stylus. Coupled with a physical paddle or desk for force feedback, the CTD provides an intuitive text input interface similar to using a pen and paper.},
 author = {Frees, S. and Khouri, R. and Kessler, G. D.},
 title = {Connecting the Dots: Simple Text Input in Immersive Environments},
 keywords = {text input},
 pages = {265--268},
 publisher = {{IEEE Operations Center}},
 isbn = {1-4244-0224-7},
 booktitle = {Virtual Reality Conference, 2006},
 year = {2006},
 address = {Piscataway, NJ},
 doi = {10.1109/VR.2006.36}
}


@article{Finstad.2006,
 abstract = {The System Usability Scale (SUS) was administered verbally to native English and non-native English speakers for several internally deployed applications. It was found that a significant proportion of non-native English speakers failed to understand the word {\textquotedbl}cumbersome{\textquotedbl} in Item 8 of the SUS (that is, {\textquotedbl}I found the system to be very cumbersome to use.{\textquotedbl}) This finding has implications for reliability and validity when the questionnaire is distributed electronically in multinational usability efforts.},
 author = {Finstad, Kraig},
 year = {2006},
 title = {The System Usability Scale and Non-native English Speakers},
 url = {http://dl.acm.org.eaccess.ub.tum.de/citation.cfm?id=2835531.2835535},
 keywords = {international usability;language;multicultural;multinational;questionnaire;survey;SUS;system usability scale;usability findings;usability methods},
 pages = {185--188},
 volume = {1},
 number = {4},
 issn = {1931-3357},
 journal = {J. Usability Studies}
}


@book{Evans.1999,
 abstract = {We have developed a software tool, VType, that enables a user wearing virtual reality gloves to enter text while in a virtual world. We present techniques to convert noisy data representing nger movements from the gloves into cleaner signals and determine the nger presses. Since each nger press corresponds to more than one symbol, we then use an algorithm for resolving ambiguity on such overloaded keyboard. We demonstrate that accurate text entry is possible using Vtype.},
 author = {Evans, Francine and Skiena, Steven and Varshney, Amitabh},
 year = {1999},
 title = {VType: Entering Text in a Virtual World},
 keywords = {fine gesture recognition;overloaded keyboards;text entry systems;user interfaces;virtual reality gloves}
}


@book{ECMAInternational.2017,
 abstract = {JSON is a lightweight, text-based, language-independent syntax for defining data interchange formats. It was derived from the ECMAScript programming language, but is programming language independent. JSON defines a small set of structuring rules for the portable representation of structured data.},
 author = {{ECMA International}},
 year = {2017},
 title = {Standard ECMA-404: The JSON Data Interchange Syntax},
 url = {https://ecma-international.org/publications/standards/Ecma-404.htm},
 keywords = {javascript;JSON},
 urldate = {24.06.2019},
 edition = {2},
 originalyear = {2013}
}


@article{Zhang.2015,
 abstract = {Increasing sources of sensor measurements and prior knowledge have become available for indoor localization on smartphones. How to effectively utilize these sources for enhancing localization accuracy is an important yet challenging problem. In this paper, we present an area state-aided localization algorithm that exploits various sources of information. Specifically, we introduce the concept of area state to indicate the area where the user is on an indoor map. The position of the user is then estimated using inertial measurement unit (IMU) measurements with the aid of area states. The area states are in turn updated based on the position estimates. To avoid accumulated errors of IMU measurements, our algorithm uses WiFi received signal strength indicator (RSSI) to indicate the vicinity of the user to the routers. The experiment results show that our system can achieve satisfactory localization accuracy in a typical indoor environment.},
 author = {Zhang, Kaiqing and Hu, Hong and Dai, Wenhan and Shen, Yuan and Win, Moe Z.},
 year = {2015},
 title = {Indoor Localization Algorithm For Smartphones},
 volume = {abs/1503.07628},
 journal = {CoRR}
}


