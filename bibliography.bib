% This file was created with Citavi 6.3.0.0

@inproceedings{Afonso.2017,
 abstract = {How does the virtual representation of the user's hands influence the performance on a button selection task performed in a tabletbased interaction within an immersive virtual environment? To answer this question, we asked 55 participants to use three conditions: no-hand avatar, realistic avatar and translucent avatar. The participants were faster but made slightly more errors while using the no-avatar condition, and considered easier to perform the task with the translucent avatar.},
 author = {Afonso, Luis and Dias, Paulo and Ferreira, Carlos and Santos, Beatriz Sousa},
 title = {Effect of hand-avatar in a selection task using a tablet as input device in an immersive virtual environment},
 keywords = {button selection task;hand-avatar;immersive virtual environment;input device;mobile device;User study},
 pages = {247--248},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 booktitle = {2017 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2017.7893364}
}


@proceedings{Tan.2011,
 year = {2011},
 title = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450302685},
 series = {ACM Digital Library},
 editor = {Tan, Desney},
 institution = {{Association for Computing Machinery}},
 doi = {10.1145/1979742}
}


@inproceedings{Steed.2013,
 abstract = {With the increasing power of mobile CPUs and GPUs, it is becoming tractable to integrate all the components of an interactive, immersive virtual reality system onto a small mobile device. We present a demonstration of a head-mounted display system integrated onto an iPhone-based platform. In building this demonstration we tackled two main problems. First, how to integrate the userinterface, utilizing the phone itself as an unseen touch interface. Second, how to integrate multiple inertial measuring units to facilitate user interaction. The resulting system indicates the practicality of mobile virtual reality systems based on smartphones.},
 author = {Steed, Anthony and Julier, Simon},
 title = {Design and implementation of an immersive virtual reality system based on a smartphone platform},
 keywords = {3D user interaction;head-mounted display;Mobile virtual reality;selection tasks},
 pages = {43--46},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-6098-2},
 editor = {L{\'e}cuyer, Anatole},
 booktitle = {2013 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2013},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2013.6550195}
}


@inproceedings{Lipari.2015,
 abstract = {We integrated touch menus into a cohesive smartphone-based VR controller. Smartphone touch surfaces offer new interaction styles and also aid VR interaction when tracking is absent or imprecise

or when users have limited arm mobility or fatigue. In Handymenu, a touch surface is split into two areas: one for menu interaction and the other for spatial interactions such as VR object selection, manipulation, navigation, or parameter adjustment. Users in our studies transitioned between the two areas and performed nested, repeated selections. A formal experiment included VR object selection (ray and touch), menu selection (ray and touch), menu layout (pie and grid), as well as touch and visual feedback sizes in some cases (two levels each).},
 author = {Lipari, Nicholas G. and Borst, Christoph W.},
 title = {Handymenu: Integrating menu selection into a multifunction smartphone-based VR controller},
 keywords = {3DTV;Menus;Smartphone;Touch;Virtual reality},
 pages = {129--132},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-6886-5},
 editor = {Lindeman, Rob},
 booktitle = {2015 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2015},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2015.7131737}
}


@proceedings{Lindeman.2015,
 year = {2015},
 title = {2015 IEEE Symposium on 3D User Interfaces (3DUI): 23 - 24 March 2015, Arles, France},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6886-5},
 editor = {Lindeman, Rob},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE Symposium on 3D User Interfaces} and 3DUI and {IEEE 3DUI Symposium}}
}


@proceedings{Lecuyer.2013,
 year = {2013},
 title = {2013 IEEE Symposium on 3D User Interfaces (3DUI): 16 - 17 March 2013, Orlando, Florida, USA},
 keywords = {Benutzeroberfl{\"a}che;Congresses;Dreidimensionales Bild;Erweiterte Realit{\"a}t;Three-dimensional display systems;User interfaces (Computer systems);Virtual reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6098-2},
 editor = {L{\'e}cuyer, Anatole},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Symposium on 3D User Interfaces} and 3DUI}
}


@inproceedings{Katzakis.2010,
 abstract = {Conventional input devices such as the mouse and keyboard lack in intuitiveness when it comes to 3D manipulation tasks. In this paper, we explore the use of accelerometer and magnetometer equipped mobile phones as 3-DOF controllers in a 3D rotation task. We put the mobile phone up against the established standards, a mouse and a touch pen and compare their performance. Our preliminary evaluation indicates that for this type of task, with only 5 minutes of practice the mobile device is significantly faster than both the mouse and the touch pen.},
 author = {Katzakis, Nicholas and Hori, Masahiro},
 title = {Mobile devices as multi-DOF controllers},
 keywords = {3D input;accelerometer;controller;interaction;magnetometer;rotation;touchscreen},
 pages = {139--140},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4244-6846-1},
 editor = {Hachet, Martin},
 booktitle = {IEEE Symposium on 3D User Interfaces (3DUI), 2010 ; Waltham, Massachusetts, USA, 20 - 21 March 2010},
 year = {2010},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2010.5444700}
}


@proceedings{IEEESymposiumon3DUserInterfaces.2017,
 year = {2017},
 title = {2017 IEEE Symposium on 3D User Interfaces (3DUI): Proceedings : March 18-19, 2017, Los Angeles, CA, USA},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 institution = {{IEEE Symposium on 3D User Interfaces} and {IEEE Computer Society} and 3DUI}
}


@proceedings{Hepper.2012,
 year = {2012},
 title = {2012 IEEE International Conference on Consumer Electronics - Berlin (ICCE-Berlin 2012): Berlin, Germany, 3 - 5 September 2012},
 keywords = {ilmpub},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-1547-0},
 editor = {Hepper, Dietmar},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Consumer Electronics Society} and {IEEE International Conference on Consumer Electronics - Berlin} and {IEEE ICCE-Berlin}}
}


@proceedings{Thomas.2016,
 year = {2016},
 title = {2016 IEEE Symposium on 3D User Interfaces (3DUI): Greenville, South Carolina, USA, 19-20 March 2016 : proceedings},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1},
 editor = {Thomas, Bruce H. and Lindeman, Rob and Marchal, Maud},
 institution = {{IEEE Symposium on 3D User Interfaces} and {Institute of Electrical and Electronics Engineers} and 3DUI and {IEEE Virtual Reality} and {IEEE VR}}
}


@proceedings{Hachet.2010,
 year = {2010},
 title = {IEEE Symposium on 3D User Interfaces (3DUI), 2010 ; Waltham, Massachusetts, USA, 20 - 21 March 2010},
 keywords = {Benutzeroberfl{\"a}che;Congresses;Dreidimensionales Bild;Erweiterte Realit{\"a}t;Three-dimensional display systems;User interfaces (Computer systems);Virtual reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4244-6846-1},
 editor = {Hachet, Martin},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Symposium on 3D User Interfaces} and 3DUI}
}


@inproceedings{Graf.2012,
 abstract = {This paper presents a concept for the use of a smartphone as a handheld input device for the interaction with 3D visualizations and presentations. Applications are mainly in the area of exhibitions and museums to enable visitors to interact with certain exhibits. Moreover applications for business presentations or computer games are possible. The usability of motion and position sensors in modern smartphones for the purpose of 3D navigation is examined. An algorithm to prevent from position drifting on short distances is developed. Finally a demo application presents the navigation concept and the ability for multiple users to simultaneously interact with the same visualization.},
 author = {Graf, Henning and Jung, Klaus},
 title = {The smartphone as a 3D input device},
 keywords = {3D navigation;human-device interaction;mobile devices},
 pages = {254--257},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-1547-0},
 editor = {Hepper, Dietmar},
 booktitle = {2012 IEEE International Conference on Consumer Electronics - Berlin (ICCE-Berlin 2012)},
 year = {2012},
 address = {Piscataway, NJ},
 doi = {10.1109/ICCE-Berlin.2012.6336487}
}


@misc{Dias.2018,
 abstract = {Gamepads and 3D controllers are the main controllers used in most Virtual Environments. Despite being simple to use, these input devices have a number of limitations as fixed layout and difficulty to remember the mapping between buttons and functions. Mobile devices present interesting characteristics that might be valuable in immersive environments: more flexible interfaces, touchscreen combined with onboard sensors that allow new interaction and easy acceptance since these devices are used daily by most users. The work described in this article proposes a solution that uses mobile devices to interact with Immersive Virtual Environments for selection and navigation tasks. The proposed solution uses the mobile device camera to track the Head-Mounted-Display position and present a virtual representation of the mobile device screen; it was tested using an Immersive Virtual Museum as use case. Based on this prototype, a study was performed to compare controller based and mobile based interaction for navigation and selection showing that using mobile devices is viable in this context and offers interesting interaction opportunities.},
 author = {Dias, Paulo and Afonso, Luis and Eliseu, S{\'e}rgio and Santos, Beatriz Sousa},
 year = {2018},
 title = {Mobile devices for interaction in immersive virtual environments},
 url = {http://dl.acm.org.eaccess.ub.tum.de/ft_gateway.cfm?id=3206526&type=pdf},
 keywords = {3D interaction;immersive virtual reality;mobile devices},
 publisher = {ACM},
 isbn = {978-1-4503-5616-9},
 doi = {10.1145/3206505.3206526}
}


@incollection{Deller.2011,
 abstract = {Large, public displays are increasingly popular in today's society. For the most part, however, these displays are purely used for information or multimedia presentation, without the possibility of interaction for viewers. On the other hand, personal mobile devices are becoming more and more ubiquitous. Though there are efforts to combine large screens with mobile devices, the approaches are mostly focused on mobiles as control devices, or they are fitted to specific applications. In this paper, we present the ModControl framework, a configurable, modular communication structure that enables large screen applications to connect with personal mobile devices and request a set of configurable modules, utilizing the device as a personalized mobile interface. The main application can easily make use of the highly sophisticated interaction features provided by modern mobile phones. This facilitates new, interactive appealing visualizations that can be actively controlled with an intuitive, unified interface by single or multiple users.},
 author = {Deller, Matthias and Ebert, Achim},
 title = {ModControl -- Mobile Phones as a Versatile Interaction Device for Large Screen Applications},
 keywords = {Distributed interfaces;Input devices and strategies;Interaction framework;User-Centered Design},
 pages = {289--296},
 bookpagination = {page},
 volume = {6947},
 publisher = {Springer},
 isbn = {978-3-642-23770-6},
 series = {Lecture Notes in Computer Science},
 editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
 booktitle = {Human-computer interaction - INTERACT 2011},
 year = {2011},
 address = {Berlin},
 doi = {10.1007/978-3-642-23771-3_22}
}


@book{Campos.2011,
 year = {2011},
 title = {Human-computer interaction - INTERACT 2011: 13th IFIP TC 13 international conference, Lisbon, Portugal, September 5 - 9, 2011 ; proceedings, part II},
 keywords = {Artificial intelligence;Computer science;Education;Information systems;Software engineering},
 address = {Berlin},
 volume = {6947},
 publisher = {Springer},
 isbn = {978-3-642-23770-6},
 series = {Lecture Notes in Computer Science},
 editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
 doi = {10.1007/978-3-642-23771-3}
}


@misc{Berge.20161026,
 abstract = {3D Virtual Environments (3DVE) come up as a good solution to transmit knowledge in a museum exhibit. In such contexts, providing easy to learn and to use interaction techniques which facilitate the handling inside a 3DVE is crucial to maximize the knowledge transfer. We took the opportunity to design and implement a software platform for explaining the behavior of the Telescope Bernard-Lyot to museum visitors on top of the Pic du Midi. Beyond the popularization of a complex scientific equipment, this platform constitutes an open software environment to easily plug different 3D interaction techniques. Recently, popular use of a smartphones as personal handled computer lets us envision the use of a mobile device as an interaction support with these 3DVE. Accordingly, we design and propose how to use the smartphone as a tangible object to navigate inside a 3DVE. In order to prove the interest in the use of smartphones, we compare our solution with available solutions: keyboard-mouse and 3D mouse. User experiments confirmed our hypothesis and particularly emphasizes that visitors find our solution more attractive and stimulating. Finally, we illustrate the benefits of our software framework by plugging alternative interaction techniques for supporting selection and manipulation task in 3D.},
 author = {Berg{\'e}, Louis-Pierre and Perelman, Gary and Hamelin, Adrien and Raynal, Mathieu and Sanza, C{\'e}dric and Houry-Panchetti, Minica and Cabanac, R{\'e}mi and Dubois, Emmanuel},
 year = {2014},
 title = {Smartphone Based 3D Navigation Techniques in an Astronomical Observatory Context: Implementation and Evaluation in a Software Platform},
 url = {https://hal.archives-ouvertes.fr/hal-01387801/document},
 keywords = {3D environment;3D navigation;Experiment;Interaction with smartphone;Interactive visualization;Museum exhibit;Software platform}
}


@inproceedings{Benzina.2011,
 abstract = {We introduce a one-handed travel technique for virtual

environments (VE), we call Phone-Based Motion

Control. The travel technique uses a mobile phone with

integrated sensors as a 3D spatial input device. We

benefit from the touch capability to change the

viewpoint translation in the VE, while the orientation of

the viewpoint in the VE is controlled by the built-in

sensors. The travel interaction clearly distinguishes

between translation (touch based translation control)

and rotation (steer based rotation control), putting

each set of degrees of freedom to a separate

interaction technique.

This work examines how many degrees of freedom are

needed to perform the travel task as easy as possible.

It also investigates different mapping functions

between the user's actions and the viewpoint reactions

in the VR. For that purpose, four metaphors are

developed for the steer based rotation control

technique. The results of the user study indicate the

trend that 4 DOF metaphors perform best, and that the

usage of a mobile roll to control the viewpoint is the

desired mapping.},
 author = {Benzina, Amal and Toennis, Marcus and Klinker, Gudrun and Ashry, Mohamed},
 title = {Phone-based motion control in VR},
 keywords = {degree of freedom;interaction;Navigation;Travel;User study},
 pages = {1519},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {9781450302685},
 series = {ACM Digital Library},
 editor = {Tan, Desney},
 booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 year = {2011},
 address = {New York, NY},
 doi = {10.1145/1979742.1979801}
}


@inproceedings{Bauer.2011,
 abstract = {Due to their size, large high-resolution screens have become popular display devices used in collaborative scenarios. However, traditional interaction methods based on combinations of computer mice and keyboards often do not scale to the number of users or the size of the display. Modern smart phones featuring multi-modal input/output by means of built-in cameras, acceleration sensors, internet capability, touch screens and considerable memory offer a way to address these issues. In the last couple of years they have become common everyday life gadgets. In this paper we conduct an extensive user study comparing the experience of test candidates when using traditional input devices and metaphors with the one when using new smart phone based techniques, like multi-modal drag and tilt. Candidates were asked to complete various 2D and 3D interaction tasks relevant for most applications on a large, monitor-based, high-resolution tiled wall system. Our study evaluates both user performance and satisfaction, identifying strengths and weaknesses of the researched interaction methods in specific tasks. By breaking these tasks down into a well-defined set of subtasks the results of each task are comparable to each other and can be classified by subtask and use case. Results reveal a superior performance of users in certain tasks when using the new interaction techniques. Even first-time users were able to complete a task faster with the smart phone than with traditional devices.},
 author = {Bauer, Jens and Thelen, Sebastian and Ebert, Achim},
 title = {Using smart phones for large-display interaction},
 keywords = {Collaboration;evaluation methods/usability evaluation;interaction with small or large displays;mobility/mobile accessibility/mobile devices;multi-modal interfaces},
 pages = {42--47},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4577-1655-3},
 booktitle = {International Conference on User Science and Engineering (i-USEr), 2011},
 year = {2011},
 address = {Piscataway, NJ},
 doi = {10.1109/iUSEr.2011.6150533}
}


@article{Ballagas.2006,
 author = {Ballagas, R. and Borchers, J. and Rohs, M. and Sheridan, J. G.},
 year = {2006},
 title = {The Smart Phone: A Ubiquitous Input Device},
 pages = {70--77},
 pagination = {page},
 volume = {5},
 number = {1},
 issn = {1536-1268},
 journal = {IEEE Pervasive Computing},
 doi = {10.1109/MPRV.2006.18}
}


@inproceedings{Grandi.2016,
 abstract = {We present a 3D user interface for collaborative manipulation of

three-dimensional objects in virtual environments. It maps inertial sensors, touch screen and physical buttons of a mobile phone into well-known gestures to alter the position, rotation and scale of virtual

objects. As these transformations require the control of multiple degrees of freedom (DOFs), collaboration is proposed as a solution to coordinate the modification of each and all the available DOFs. Users are free to decide their own manipulation roles. All virtual elements are displayed in a single shared screen, which is handy to aggregate multiple users in the same physical space.},
 author = {Grandi, Jeronimo G. and Berndt, Iago and Debarba, Henrique G. and Nedel, Luciana and Maciel, Anderson},
 title = {Collaborative 3D manipulation using mobile phones},
 keywords = {3D interaction;Collaboration;controller;mobile devices},
 pages = {279--280},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1},
 editor = {Thomas, Bruce H. and Lindeman, Rob and Marchal, Maud},
 booktitle = {2016 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2016},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2016.7460079}
}


@proceedings{UniversitiTeknologiMARA.2011,
 year = {2011},
 title = {International Conference on User Science and Engineering (i-USEr), 2011: Nov. 29 2011 - Dec. 1 2011, Selangor, Malaysia ; proceedings},
 keywords = {Computer software;Congresses;Human factors;Human-computer interaction},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4577-1655-3},
 institution = {{Universiti Teknologi MARA} and {International Conference on User Science and Engineering} and i-USEr}
}


