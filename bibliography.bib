% This file was created with Citavi 6.3.0.0

@inproceedings{Afonso.2017,
 abstract = {How does the virtual representation of the user's hands influence the performance on a button selection task performed in a tabletbased interaction within an immersive virtual environment? To answer this question, we asked 55 participants to use three conditions: no-hand avatar, realistic avatar and translucent avatar. The participants were faster but made slightly more errors while using the no-avatar condition, and considered easier to perform the task with the translucent avatar.},
 author = {Afonso, Luis and Dias, Paulo and Ferreira, Carlos and Santos, Beatriz Sousa},
 title = {Effect of hand-avatar in a selection task using a tablet as input device in an immersive virtual environment},
 keywords = {button selection task;hand-avatar;immersive virtual environment;input device;mobile device;User study},
 pages = {247--248},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 booktitle = {2017 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2017.7893364}
}


@phdthesis{Kamm.2018,
 abstract = {One of the primary goals in the mixed reality continuum is it to create interaction techniques that feel as natural and immersive as possible. However, it is unclear to which degree this naturalism can be achieved and whether it is always a desirable goal.In the past, many selection techniques have prioritized high accuracy and usability but on modern hardware devices, like the leap motion controller, more natural and bare-handbased interactions have been designed. What is still lacking is a test of these techniques outside of desktop-based environments.With this thesis, we contribute to the discussion around new selection methods in three ways. (1) We introduce the HeadRay, a relative pointing technique for VR, which does not depend on the precise position of the hand, and which therefore is very well suited for use with gesture-based input devices like the Myo armband. We show in a user survey that our technique is a well-working selection method that substantially increases immersion. At the same time, it still suffers from inconsistency in the gesture recognition of the Myo that hamper the usability. (2) We show that it is possible to design test scenarios that fully utilize the virtual space around the user and still allow to make measurements that highly correlate with objective performance and subjective ratings by the user. (3) We demonstrate that the Depth Marker proposed by Grossman et al. is a good way to handle occlusion in VR.},
 author = {Kamm, Clemens},
 year = {2018},
 title = {Precision of Pointing with Myo: A Comparison of Controller- and Gesture-Based Selection in Virtual Reality},
 url = {https://drive.google.com/file/d/10LiDxn3zx_S8gCAcJN7eC8iK1a8aKflf/view},
 address = {Munich},
 urldate = {2019-06-20},
 school = {{Technische Universit{\"a}t M{\"u}nchen}},
 type = {Master's Thesis}
}


@inproceedings{Katsuragawa.2016,
 abstract = {We describe the design and evaluation of a freehand, smartwatchbased, mid-air pointing and clicking interaction technique, called Watchpoint. Watchpoint enables a user to point at a target on a nearby large display by moving their arm. It also enables target selection through a wrist rotation gesture. We validate the use of Watchpoint by comparing its performance with two existing techniques: Myopoint, which uses a specialized forearm mounted motion sensor, and a camera-based (Vicon) motion capture system. We show that Watchpoint is statistically comparable in speed and error rate to both systems and, in fact, outperforms in terms of error rate for small (high Fitts's ID) targets. Our work demonstrates that a commodity smartwatch can serve as an effective pointing device in ubiquitous display environments.},
 author = {Katsuragawa, Keiko and Pietroszek, Krzysztof and Wallace, James R. and Lank, Edward},
 title = {Watchpoint: Freehand Pointing with a Smartwatch in a Ubiquitous Display Environment},
 keywords = {Large displays;pointing;smartwatch;wearable},
 pages = {128--135},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {978-1-4503-4131-8},
 editor = {Buono, Paolo and Lanzilotti, Rosa and Matera, Maristella},
 booktitle = {AVI '16: Proceedings of the International Working Conference on Advanced Visual Interfaces},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/2909132.2909263}
}


@inproceedings{Katzakis.2010,
 abstract = {Conventional input devices such as the mouse and keyboard lack in intuitiveness when it comes to 3D manipulation tasks. In this paper, we explore the use of accelerometer and magnetometer equipped mobile phones as 3-DOF controllers in a 3D rotation task. We put the mobile phone up against the established standards, a mouse and a touch pen and compare their performance. Our preliminary evaluation indicates that for this type of task, with only 5 minutes of practice the mobile device is significantly faster than both the mouse and the touch pen.},
 author = {Katzakis, Nicholas and Hori, Masahiro},
 title = {Mobile devices as multi-DOF controllers},
 keywords = {3D input;accelerometer;controller;interaction;magnetometer;rotation;touchscreen},
 pages = {139--140},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4244-6846-1},
 editor = {Hachet, Martin},
 booktitle = {IEEE Symposium on 3D User Interfaces (3DUI), 2010 ; Waltham, Massachusetts, USA, 20 - 21 March 2010},
 year = {2010},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2010.5444700}
}


@proceedings{Kim.2015,
 year = {2015},
 title = {CHI 2015 crossings: CHI 2015 ; proceedings of the 33rd Annual CHI Conference on Human Factors in Computing Systems ; April 18 - 23, 2015, Seoul, Republic of Korea},
 keywords = {Benutzeroberfl{\"a}che;Mensch-Maschine-Schnittstelle},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450331456},
 editor = {Kim, Jinwoo},
 institution = {{Association for Computing Machinery} and {Annual CHI Conference on Human Factors in Computing Systems} and CHI},
 doi = {10.1145/2702123}
}


@phdthesis{Koetsier.2016,
 abstract = {Vampires is a framework that assists in finding the optimal combination of resources to use for execution of a set of independent tasks in a heterogeneous cloud environment. This thesis discusses the decisions made during the development of a web-based user interface for Vampires. This includes a thorough evaluation of client-side JavaScript frameworks, resulting in the choice of AngularJS to use as the basis of the Vampires user interface.},
 author = {Koetsier, Jaap},
 year = {2016},
 title = {Evaluation of JavaScript frame-works for the development of a web-based user interface for Vampires}
}


@book{Latorre.2009,
 abstract = {This book comprises a collection of chapters concerning novel research on Human-Computer Interaction. Chapters correspond to high-quality selected papers from Interaccion 2007, the 8th annual conference on Human-Computer Interaction held in Zaragoza, Spain, as part of the II CEDI, the second edition of the Spanish Conference on Informatics, the biggest and most important in Spain. This book is particularly aimed at researchers, advanced HCI students and practitioners. It includes promising works that range from cutting edge paradigms such as Semantic Web Interfaces, Natural Language Processing and Mobile Interaction to new trends and interface-engineering techniques such as User-Centred Design, Usability, Accessibility, Development Methodologies and Emotional User Interfaces. It also provides a specialized learning reference for advanced lectures, involving topics such as Human Factors, Collaborative User Interfaces and Model-based User Interface Design.},
 year = {2009},
 title = {New Trends on Human-Computer Interaction: Research, Development, New Tools and Methods},
 keywords = {Computer science;Information systems;Mensch-Maschine-Kommunikation;Multimedia systems;Software engineering},
 address = {London},
 publisher = {{Springer-Verlag London}},
 isbn = {978-1-84882-351-8},
 editor = {Latorre, Pedro M. and {Granollers Saltiveri}, Antoni and Mac{\'i}as, Jos{\'e} A.},
 doi = {10.1007/978-1-84882-352-5}
}


@proceedings{Lecuyer.2013,
 year = {2013},
 title = {2013 IEEE Symposium on 3D User Interfaces (3DUI): 16 - 17 March 2013, Orlando, Florida, USA},
 keywords = {Benutzeroberfl{\"a}che;Congresses;Dreidimensionales Bild;Erweiterte Realit{\"a}t;Three-dimensional display systems;User interfaces (Computer systems);Virtual reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6098-2},
 editor = {L{\'e}cuyer, Anatole},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Symposium on 3D User Interfaces} and 3DUI}
}


@proceedings{Leong.2014,
 abstract = {Annotation},
 year = {2014},
 title = {Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures the Future of Design},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450306539},
 editor = {Leong, Tuck},
 doi = {10.1145/2686612}
}


@proceedings{Lindeman.2015,
 year = {2015},
 title = {2015 IEEE Symposium on 3D User Interfaces (3DUI): 23 - 24 March 2015, Arles, France},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6886-5},
 editor = {Lindeman, Rob},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE Symposium on 3D User Interfaces} and 3DUI and {IEEE 3DUI Symposium}}
}


@inproceedings{Lipari.2015,
 abstract = {We integrated touch menus into a cohesive smartphone-based VR controller. Smartphone touch surfaces offer new interaction styles and also aid VR interaction when tracking is absent or impreciseor when users have limited arm mobility or fatigue. In Handymenu, a touch surface is split into two areas: one for menu interaction and the other for spatial interactions such as VR object selection, manipulation, navigation, or parameter adjustment. Users in our studies transitioned between the two areas and performed nested, repeated selections. A formal experiment included VR object selection (ray and touch), menu selection (ray and touch), menu layout (pie and grid), as well as touch and visual feedback sizes in some cases (two levels each).},
 author = {Lipari, Nicholas G. and Borst, Christoph W.},
 title = {Handymenu: Integrating menu selection into a multifunction smartphone-based VR controller},
 keywords = {3DTV;Menus;Smartphone;Touch;Virtual reality},
 pages = {129--132},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-6886-5},
 editor = {Lindeman, Rob},
 booktitle = {2015 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2015},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2015.7131737}
}


@proceedings{Mark.2017,
 year = {2017},
 title = {Explore, innovate, inspire: CHI 2017 : May 6-11, Denver, CO, USA},
 keywords = {Benutzeroberfl{\"a}che;Mensch-Maschine-Schnittstelle},
 address = {New York, NY},
 publisher = {{Association for Computing Machinery Inc. (ACM)}},
 isbn = {9781450346559},
 editor = {Mark, Gloria and Fussell, Susan and Lampe, Cliff and schraefel, m.c and Hourcade, Juan Pablo and Appert, Caroline and Wigdor, Daniel},
 institution = {CHI and {Association for Computing Machinery} and {ACM CHI} and {ACM CHI Conference on Human Factors in Computing Systems} and {CHI Conference on Human Factors in Computing Systems}},
 doi = {10.1145/3025453}
}


@inproceedings{McGill.2015,
 abstract = {We identify usability challenges facing consumers adopting Virtual Reality (VR) head-mounted displays (HMDs) in a survey of 108 VR HMD users. Users reported significant issues in interacting with, and being aware of their real-world context when using a HMD. Building upon existing work on blending real and virtual environments, we performed three design studies to address these usability concerns. In a typing study, we show that augmenting VR with a view of reality significantly corrected the performance impairment of typing in VR. We then investigated how much reality should be incorporated and when, so as to preserve users' sense of presence in VR. For interaction with objects and peripherals, we found that selectively presenting reality as users engaged with it was optimal in terms of performance and users' sense of presence. Finally, we investigated how this selective, engagement-dependent approach could be applied in social environments, to support the user's awareness of the proximity and presence of others.},
 author = {McGill, Mark and Boland, Daniel and Murray-Smith, Roderick and Brewster, Stephen},
 title = {A Dose of Reality: Overcoming Usability Challenges in VR Head-Mounted Displays},
 keywords = {Augmented Virtuality;Engagement;Virtual reality},
 pages = {2143--2152},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {9781450331456},
 editor = {Kim, Jinwoo},
 booktitle = {CHI 2015 crossings},
 year = {2015},
 address = {New York, NY},
 doi = {10.1145/2702123.2702382}
}


@book{Mould.2010,
 year = {2010},
 title = {Graphics interface 2010: Conference ; GI 2010] ; Ottawa, Ontario, Canada, 31 May - 2 June 2010 ; proceedings},
 address = {Mississauga, Ontario},
 publisher = {{Canadian Information Processing Society (CIPS)}},
 isbn = {978-1-56881-712-5},
 editor = {Mould, David and No{\"e}l, Sylvie}
}


@inproceedings{Pietroszek.2014,
 abstract = {We introduce and formally evaluate smartcasting: a smartphone-based Ray Casting implementation for 3D environments presented on large, public, autostereoscopic displays. By utilizing a smartphone as an input device, smartcasting enables ``walk up and use'' interaction with large displays, without the need for expensive tracking systems or specialized pointing devices. Through an empirical validation we show that the performance and precision of smartcasting is comparable to a Wiimotebased raycasting implementation, without requiring specialized hardware or high-precision cameras to enable user interaction.},
 author = {Pietroszek, Krzysztof and Kuzminykh, Anastasia and Wallace, James R. and Lank, Edward},
 title = {Smartcasting: A Discount 3D Interaction Technique  for Public Displays},
 keywords = {3D displays;3D environnent;3D interaction;interaction technique;mobile interaction;raycasting},
 pages = {119--128},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {978-1-4503-0653-9},
 editor = {{Tuck Wah Leong}},
 booktitle = {Proceedings of the 26th Australian Computer-Human Interaction Conference  on Designing Futures - the Future of Design, OZCHI '14, Sydney,  New South Wales, Australia, December 2-5, 2014},
 year = {2014},
 doi = {10.1145/2686612.2686629}
}


@proceedings{Rekimoto.2016,
 year = {2016},
 title = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-4189-9},
 series = {UIST '16},
 editor = {Rekimoto, Jun and Igarashi, Takeo}
}


@inproceedings{Rhoton.2002,
 abstract = {Symbolic  input, including text and numeric  input, can be an important user task in applications of virtual environments (VEs). However, very little research has been performed to  support this  task in immersive VEs. This paper presents the results of an empirical  evaluation of four text input techniques for immersive VEs. The techniques include the Pinch Keyboard (a typing emulation technique using pinch gloves), a one- hand chord keyboard, a soft keyboard using a pen {\&} tablet, and speech. The experiment measured both task performance and usability characteristics of the four techniques. Results indicate  that the speech technique is the  fastest, while the pen {\&} tablet keyboard produces  the  fewest  errors. However,  no  single technique exhibited high levels of performance, usability  and user satisfaction.},
 author = {Rhoton, Christopher J. and Bowman, Doug A. and Pinho, Marcio S.},
 title = {Text Input Techniques for Immersive Virtual Environments: an Empirical Comparison},
 pages = {2154--2158},
 bookpagination = {page},
 publisher = {{SAGE Publications}},
 editor = {{Human Factors} and {Ergonomics Society. Annual meeting}},
 booktitle = {Proceedings of the Human Factors and Ergonomics Society: 46th Annual Meeting, Baltimore, Maryland, September 30 - October 4, 2002 : Bridging Fundamentals {\&} New Opportunities},
 year = {2002},
 address = {Santa Monica, Calif.}
}


@inproceedings{Schops.2014,
 abstract = {We present a direct monocular visual odometry system which runs in real-time on a smartphone. Being a direct method, it tracks and maps on the images themselves instead of extracted features such as keypoints. New images are tracked using direct image alignment, while geometry is represented in the form of a semi-dense depth map. Depth is estimated by filtering over many small-baseline, pixel-wise stereo comparisons. This leads to significantly less outliers and allows to map and use all image regions with sufficient gradient, including edges. We show how a simple world model for AR applications can be derived from semi-dense depth maps, and demonstrate the practical applicability in the context of an AR application in which simulated objects can collide with real geometry.},
 author = {Schops, Thomas and Engel, Jakob and Cremers, Daniel},
 title = {Semi-dense visual odometry for AR on a smartphone},
 pages = {145--150},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4799-6184-9},
 editor = {Julier, Simon},
 booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2014},
 year = {2014},
 address = {Piscataway, NJ},
 doi = {10.1109/ISMAR.2014.6948420}
}


@inproceedings{Shibata.2016,
 abstract = {Emerging ultra-small wearables like smartwatches pose a design challenge for touch-based text entry. This is due to the ``fat-finger problem,'' wherein users struggle to select elements much smaller than their fingers. To address this challenge, we developed DriftBoard, a panning-based text entry technique where the user types by positioning a movable qwerty keyboard on an interactive area with respect to a fixed cursor point. In this paper, we describe the design and implementation of DriftBoard and report results of a user study on a watch-size touchscreen. The study compared DriftBoard to two ultra-small keyboards, ZoomBoard (tapping-based) and Swipeboard (swiping-based). DriftBoard performed comparably (no significant difference) to ZoomBoard in the major metrics of text entry speed and error rate, and outperformed Swipeboard, which suggests that panning-based typing is a promising input method for text entry on ultra-small touchscreens.},
 author = {Shibata, Tomoki and Afergan, Daniel and Kong, Danielle and Yuksel, Beste F. and MacKenzie, I. Scott and Jacob, Robert J.K.},
 title = {DriftBoard: A Panning-Based Text Entry Technique for Ultra-Small Touchscreens},
 url = {http://doi.acm.org.eaccess.ub.tum.de/10.1145/2984511.2984591,  [Add to Citavi project by DOI]},
 keywords = {driftboard;fat-finger problem;fixed cursor;movable keyboard;soft keyboard;text entry;ultra-small touchscreens},
 pages = {575--582},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {978-1-4503-4189-9},
 series = {UIST '16},
 editor = {Rekimoto, Jun and Igarashi, Takeo},
 booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/2984511.2984591}
}


@inproceedings{Steed.2013,
 abstract = {With the increasing power of mobile CPUs and GPUs, it is becoming tractable to integrate all the components of an interactive, immersive virtual reality system onto a small mobile device. We present a demonstration of a head-mounted display system integrated onto an iPhone-based platform. In building this demonstration we tackled two main problems. First, how to integrate the userinterface, utilizing the phone itself as an unseen touch interface. Second, how to integrate multiple inertial measuring units to facilitate user interaction. The resulting system indicates the practicality of mobile virtual reality systems based on smartphones.},
 author = {Steed, Anthony and Julier, Simon},
 title = {Design and implementation of an immersive virtual reality system based on a smartphone platform},
 keywords = {3D user interaction;head-mounted display;Mobile virtual reality;selection tasks},
 pages = {43--46},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-6098-2},
 editor = {L{\'e}cuyer, Anatole},
 booktitle = {2013 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2013},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2013.6550195}
}


@proceedings{Tan.2011,
 year = {2011},
 title = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450302685},
 series = {ACM Digital Library},
 editor = {Tan, Desney},
 institution = {{Association for Computing Machinery}},
 doi = {10.1145/1979742}
}


@proceedings{Thomas.2016,
 year = {2016},
 title = {2016 IEEE Symposium on 3D User Interfaces (3DUI): Greenville, South Carolina, USA, 19-20 March 2016 : proceedings},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1},
 editor = {Thomas, Bruce H. and Lindeman, Rob and Marchal, Maud},
 institution = {{IEEE Symposium on 3D User Interfaces} and {Institute of Electrical and Electronics Engineers} and 3DUI and {IEEE Virtual Reality} and {IEEE VR}}
}


@proceedings{TuckWahLeong.2014,
 year = {2014},
 title = {Proceedings of the 26th Australian Computer-Human Interaction Conference  on Designing Futures - the Future of Design, OZCHI '14, Sydney,  New South Wales, Australia, December 2-5, 2014},
 url = {https://doi.org/10.1145/2686612,  [Add to Citavi project by DOI]},
 publisher = {ACM},
 isbn = {978-1-4503-0653-9},
 editor = {{Tuck Wah Leong}},
 doi = {10.1145/2686612}
}


@proceedings{UniversitiTeknologiMARA.2011,
 year = {2011},
 title = {International Conference on User Science and Engineering (i-USEr), 2011: Nov. 29 2011 - Dec. 1 2011, Selangor, Malaysia ; proceedings},
 keywords = {Computer software;Congresses;Human factors;Human-computer interaction},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4577-1655-3},
 institution = {{Universiti Teknologi MARA} and {International Conference on User Science and Engineering} and i-USEr}
}


@proceedings{VR.2006,
 year = {2006},
 title = {Virtual Reality Conference, 2006: 25 - 29 March 2006, [Alexandria, VA ; proceedings},
 keywords = {Computersimulation;Virtuelle Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {{IEEE Operations Center}},
 isbn = {1-4244-0224-7},
 institution = {VR and {IEEE Computer Society} and {IEEE Virtual Reality Conference}}
}


@inproceedings{Walker.2017,
 abstract = {The rise of affordable head-mounted displays (HMDs) has raised questions about how to best design user interfaces for this technology. This paper focuses on the use of HMDs for home and office applications that require substantial text input. A physical keyboard is a familiar and effective text input device in normal desktop computing. But without additional camera technology, an HMD occludes all visual feedback about a user's hand position over the keyboard. We describe a system that assists HMD users in typing on a physical keyboard. Our system has a virtual keyboard assistant that provides visual feedback inside the HMD about a user's actions on the physical keyboard. It also provides powerful automatic correction of typing errors by extending a state-of-the-art touchscreen decoder. In a study with 24 participants, we found our virtual keyboard assistant enabled users to type more accurately on a visually-occluded keyboard. We found users wearing an HMD could type at over 40 words-per-minute while obtaining an error rate of less than 5{\%}.},
 author = {Walker, James and Li, Bochao and Vertanen, Keith and Kuhl, Scott},
 title = {Efficient Typing on a Visually Occluded Physical Keyboard},
 keywords = {decoder;head-mounted display;Physical keyboard;text entry},
 pages = {5457--5461},
 bookpagination = {page},
 publisher = {{Association for Computing Machinery Inc. (ACM)}},
 isbn = {9781450346559},
 editor = {Mark, Gloria and Fussell, Susan and Lampe, Cliff and schraefel, m.c and Hourcade, Juan Pablo and Appert, Caroline and Wigdor, Daniel},
 booktitle = {Explore, innovate, inspire},
 year = {2017},
 address = {New York, NY},
 doi = {10.1145/3025453.3025783}
}


@article{Watsen.1999,
 abstract = {A fundamental problem hindering the advancement of virtual world development is that of interaction techniques. There is contention between 2D and 3D techniques and uncertainty as to which is appropriate and when. We have developed a simple mechanism to address this problem whereby the user performs tasks appropriate to 2D interfaces with the 3Com PalmPilot handheld computer. The use of a wireless serial connection allows for unencumbered immersion in CAVE-like environments. Our implementation utilizes Bamboo, a dynamically extensible virtual environment toolkit, which enables our design to accommodate new user interfaces on the fly. We are in the early stages of analyzing these tasks and techniques for usability and efficiency. The paper reports techniques that we have implemented, and the specifics of using Bamboo and a PalmPilot for virtual world applications.},
 author = {Watsen, Kent and Darken, Rudolph P. and Capps, Michael V.},
 year = {1999},
 title = {A Handheld Computer as an Interaction Device to a Virtual Environment}
}


@online{WeelcoInc.2017,
 author = {{Weelco Inc}},
 year = {2017},
 title = {Unity Asset Store: Keyboard VR Pro},
 url = {https://assetstore.unity.com/packages/tools/input-management/keyboard-vr-pro-83708},
 keywords = {Keyboard VR Pro;Tools/Input Management},
 urldate = {2019-06-26}
}


@online{Weisel.2017,
 abstract = {Cutie Keys is an open source VR drum keyboard written in Unity/C{\#}. The goal of this project is to provide a basic drum keyboard that anyone can use in their own VR project},
 author = {Weisel, Max},
 year = {2017},
 title = {An open-source keyboard to make your own},
 url = {http://www.normalvr.com/blog/an-open-source-keyboard-to-make-your-own/},
 urldate = {2019-06-26}
}


@online{Yang.2018,
 abstract = {The team continues to work on updates to SteamVR Input, and we've made strides in the overall system and user experience. Recently, we've updated the SteamVR beta with an improved Controller Binding UI - making it simpler to rebind games that haven't implemented the new SteamVR Input API. Now it's even easier for players and developers to create and share new bindings for VR games, for any current and future controllers.},
 author = {Yang, Lawrence},
 year = {2018},
 title = {Guide: Rebinding Games for New Controllers},
 url = {https://steamcommunity.com/games/250820/announcements/detail/1697188096865619876},
 urldate = {2019-08-05}
}


@proceedings{Julier.2014,
 year = {2014},
 title = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2014: 10 - 12 Sept. 2014, Munich, Germany},
 keywords = {Erweiterte Realit{\"a}t;Mixed Reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4799-6184-9},
 editor = {Julier, Simon},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE International Symposium on Mixed and Augmented Reality} and ISMAR and {IEEE International Symposium on Mixed and Augmented Reality - Science {\&} Technology}}
}


@incollection{Jota.2010,
 abstract = {Ray-pointing  techniques  are  often  advocated  as  a  way  for  people  to interact with very large displays from several meters away. We are  interested  in  two  factors  that  can  affect  ray  pointing:  the  par-ticular technique's control type, and parallax.  Consequently,  we  tested  four  ray  pointing  variants  on  a  wall  display  that  covers  a  large  part  of  the  user's  field  of  view.  Tasks  included horizontal and vertical targeting, and tracing. Our results show  that  (a)  techniques  based  on  'rotational  control'  perform  better for targeting tasks, and (b) techniques with low parallax are best  for  tracing  tasks.  We  also  show  that  a  Fitts's  law  analysis  based  on  angles  (as  opposed  to  linear  distances)  better  approx-imates people's ray pointing performance.},
 author = {Jota, Ricardo and Nacenta, Miguel A. and Jorge, Joaquim A. and Carpendale, Sheelagh and Greenberg, Saul},
 title = {A comparison of ray pointing techniques for very large displays},
 url = {http://dl.acm.org.eaccess.ub.tum.de/ft_gateway.cfm?id=1839261&type=pdf},
 keywords = {distant pointing;image-plane;index of difficulty;ISO 9241;Large displays;parallax;ray pointing;targeting;tracing},
 urldate = {2019-06-20},
 pages = {269--276},
 bookpagination = {page},
 publisher = {{Canadian Information Processing Society (CIPS)}},
 isbn = {978-1-56881-712-5},
 editor = {Mould, David and No{\"e}l, Sylvie},
 booktitle = {Graphics interface 2010},
 year = {2010},
 address = {Mississauga, Ontario}
}


@article{JiYoungOh.2002,
 abstract = {Single Display Groupware (SDG) is a research area that focuses on providing collaborative computing environments. Traditionally, most hardware platforms for SDG support only one person interacting at any given time, which limits collaboration. In this paper, we present laser pointers as input devices that can provide concurrent input streams ideally required to the SDG environment. First, we discuss several issues related to utilization of laser

pointers and present the new concept of computer controlled laser pointers. Then we briefly present a performance evaluation of laser pointers as input devices and a baseline comparison with the mouse according to the ISO 9241-9 standard. Finally, we describe a new system that uses multiple computer controlled laser pointers as interaction devices for one or more displays. Several alternatives for distinguishing between different laser pointers are presented, and an implementation of one of them is demonstrated with SDG applications.},
 author = {{Ji-Young Oh}, Wolfgang Stuerzlinger},
 year = {2002},
 title = {Laser Pointers as Collaborative Pointing Devices},
 url = {https://ci.nii.ac.jp/naid/10017144609/en/},
 journal = {Proc. GI2002-Graphics Interface, Calgary, Canada, May}
}


@inproceedings{Jeon.2007,
 abstract = {Today's mobile phones have not only become the most representative device in the new ubiquitous computing era but also dramatically improved in terms of their multi-modal sensing and display capabilities. This advance makes the mobile phone an ideal candidate for a more natural interaction device in ubiquitous computing environment. This paper proposes techniques which use camera-equipped mobile phones for interacting with 2D and 3D applications on a tabletop display environment. The camera acts as the main sensor for a gesture-based interaction. Using the mobile phone with an interactive touch screen allows the use of techniques that move beyond single hand/finger input to improve task performance. The interaction performances of the proposed techniques and design guidelines are also described in this paper.},
 author = {Jeon, Seokhee and Kim, Gerard J. and Billinghurst, Mark},
 title = {Interacting with a Tabletop Display Using a Camera Equipped Mobile Phone},
 url = {https://link-springer-com.eaccess.ub.tum.de/content/pdf/10.1007%2F978-3-540-73107-8_38.pdf},
 pages = {336--343},
 bookpagination = {page},
 publisher = {{Springer, Berlin, Heidelberg}},
 isbn = {978-3-540-73107-8},
 editor = {Jacko, Julie A.},
 booktitle = {Human-Computer Interaction. Interaction Platforms and Techniques: 12th International Conference, HCI International 2007, Beijing, China, July 22-27, 2007, Proceedings, Part II},
 year = {2007},
 doi = {10.1007/978-3-540-73107-8_38}
}


@article{Argelaguet.2013,
 abstract = {Computer graphics applications controlled through natural gestures are gaining increasing popularity these days due to recent developments in low-cost tracking systems and gesture recognition technologies. Although interaction techniques through natural gestures have already demonstrated their benefits in manipulation, navigation and avatar-control tasks, effective selection with pointing gestures remains an open problem. In this paper we survey the state-of-the-art in 3D object selection techniques. We review important findings in human control models, analyze major factors influencing selection performance, and classify existing techniques according to a number of criteria. Unlike other components of the application's user interface, pointing techniques need a close coupling with the rendering pipeline, introducing new elements to be drawn, and potentially modifying the object layout and the way the scene is rendered. Conversely, selection performance is affected by rendering issues such as visual feedback, depth perception, and occlusion management. We thus review existing literature paying special attention to those aspects in the boundary between computer graphics and human--computer interaction.},
 author = {Argelaguet, Ferran and Andujar, Carlos},
 year = {2013},
 title = {A survey of 3D object selection techniques for virtual environments},
 pages = {121--136},
 pagination = {page},
 volume = {37},
 number = {3},
 issn = {00978493},
 journal = {Computers {\&} Graphics},
 doi = {10.1016/j.cag.2012.12.003}
}


@article{Ballagas.2006,
 author = {Ballagas, R. and Borchers, J. and Rohs, M. and Sheridan, J. G.},
 year = {2006},
 title = {The Smart Phone: A Ubiquitous Input Device},
 pages = {70--77},
 pagination = {page},
 volume = {5},
 number = {1},
 issn = {1536-1268},
 journal = {IEEE Pervasive Computing},
 doi = {10.1109/MPRV.2006.18}
}


@article{Bangor.2009,
 abstract = {The System Usability Scale (SUS) is an inexpensive, yet effective tool for assessing the usability of a product, including Web sites, cell phones, interactive voice response systems, TV applications, and more. It provides an easy-to-understand score from 0 (negative) to 100 (positive). While a 100-point scale is intuitive in many respects and allows for relative judgments, information describing how the numeric score translates into an absolute judgment of usability is not known. To help answer that question, a seven-point adjective-anchored Likert scale was added as an eleventh question to nearly 1,000 SUS surveys. Results show that the Likert scale scores correlate extremely well with the SUS scores (r=0.822). The addition of the adjective rating scale to the SUS may help practitioners interpret individual SUS scores and aid in explaining the results to non-human factors professionals.},
 author = {Bangor, Aaron and Kortum, Philip and Miller, James},
 year = {2009},
 title = {Determining What Individual SUS Scores Mean: Adding an Adjective Rating Scale},
 url = {http://dl.acm.org.eaccess.ub.tum.de/citation.cfm?id=2835587.2835589},
 keywords = {surveys;SUS;system usability scale;usability;user satisfaction},
 pages = {114--123},
 pagination = {page},
 volume = {4},
 number = {3},
 issn = {1931-3357},
 journal = {J. Usability Studies}
}


@inproceedings{Bauer.2011,
 abstract = {Due to their size, large high-resolution screens have become popular display devices used in collaborative scenarios. However, traditional interaction methods based on combinations of computer mice and keyboards often do not scale to the number of users or the size of the display. Modern smart phones featuring multi-modal input/output by means of built-in cameras, acceleration sensors, internet capability, touch screens and considerable memory offer a way to address these issues. In the last couple of years they have become common everyday life gadgets. In this paper we conduct an extensive user study comparing the experience of test candidates when using traditional input devices and metaphors with the one when using new smart phone based techniques, like multi-modal drag and tilt. Candidates were asked to complete various 2D and 3D interaction tasks relevant for most applications on a large, monitor-based, high-resolution tiled wall system. Our study evaluates both user performance and satisfaction, identifying strengths and weaknesses of the researched interaction methods in specific tasks. By breaking these tasks down into a well-defined set of subtasks the results of each task are comparable to each other and can be classified by subtask and use case. Results reveal a superior performance of users in certain tasks when using the new interaction techniques. Even first-time users were able to complete a task faster with the smart phone than with traditional devices.},
 author = {Bauer, Jens and Thelen, Sebastian and Ebert, Achim},
 title = {Using smart phones for large-display interaction},
 keywords = {Collaboration;evaluation methods/usability evaluation;interaction with small or large displays;mobility/mobile accessibility/mobile devices;multi-modal interfaces},
 pages = {42--47},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4577-1655-3},
 booktitle = {International Conference on User Science and Engineering (i-USEr), 2011},
 year = {2011},
 address = {Piscataway, NJ},
 doi = {10.1109/iUSEr.2011.6150533}
}


@inproceedings{Benzina.2011,
 abstract = {We introduce a one-handed travel technique for virtualenvironments (VE), we call Phone-Based MotionControl. The travel technique uses a mobile phone withintegrated sensors as a 3D spatial input device. Webenefit from the touch capability to change theviewpoint translation in the VE, while the orientation ofthe viewpoint in the VE is controlled by the built-insensors. The travel interaction clearly distinguishesbetween translation (touch based translation control)and rotation (steer based rotation control), puttingeach set of degrees of freedom to a separateinteraction technique.This work examines how many degrees of freedom areneeded to perform the travel task as easy as possible.It also investigates different mapping functionsbetween the user's actions and the viewpoint reactionsin the VR. For that purpose, four metaphors aredeveloped for the steer based rotation controltechnique. The results of the user study indicate thetrend that 4 DOF metaphors perform best, and that theusage of a mobile roll to control the viewpoint is thedesired mapping.},
 author = {Benzina, Amal and Toennis, Marcus and Klinker, Gudrun and Ashry, Mohamed},
 title = {Phone-based motion control in VR},
 keywords = {degree of freedom;interaction;Navigation;Travel;User study},
 pages = {1519},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {9781450302685},
 series = {ACM Digital Library},
 editor = {Tan, Desney},
 booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 year = {2011},
 address = {New York, NY},
 doi = {10.1145/1979742.1979801}
}


@incollection{Berge.,
 abstract = {3D Virtual Environments (3DVE) come up as a good solution to transmit knowledge in a museum exhibit. In such contexts, providing easy to learn and to use interaction techniques which facilitate the handling inside a 3DVE is crucial to maximize the knowledge transfer. We took the opportunity to design and implement a software platform for explaining the behavior of the Telescope Bernard-Lyot to museum visitors on top of the Pic du Midi. Beyond the popularization of a complex scientific equipment, this platform constitutes an open software environment to easily plug different 3D interaction techniques. Recently, popular use of a smartphones as personal handled computer lets us envision the use of a mobile device as an interaction support with these 3DVE. Accordingly, we design and propose how to use the smartphone as a tangible object to navigate inside a 3DVE. In order to prove the interest in the use of smartphones, we compare our solution with available solutions: keyboard-mouse and 3D mouse. User experiments confirmed our hypothesis and particularly emphasizes that visitors find our solution more attractive and stimulating. Finally, we illustrate the benefits of our software framework by plugging alternative interaction techniques for supporting selection and manipulation task in 3D.},
 author = {Berg{\'e}, Louis-Pierre and Perelman, Gary and Hamelin, Adrien and Raynal, Mathieu and Sanza, C{\'e}dric and Houry-Panchetti, Minica and Cabanac, R{\'e}mi and Dubois, Emmanuel},
 title = {Smartphone Based 3D Navigation Techniques in an Astronomical Observatory Context: Implementation and Evaluation in a Software Platform},
 url = {https://hal.archives-ouvertes.fr/hal-01387801/document},
 keywords = {3D environment;3D navigation;Experiment;Interaction with smartphone;Interactive visualization;Museum exhibit;Software platform}
}


@article{Brooke.1996,
 abstract = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for ``quick and dirty'' methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.},
 author = {Brooke, John},
 year = {1996},
 title = {SUS - A quick and dirty usability scale},
 url = {https://ci.nii.ac.jp/naid/10018890922/en/},
 journal = {Usability Evaluation in Industry}
}


@proceedings{Buono.2016,
 year = {2016},
 title = {AVI '16: Proceedings of the International Working Conference on Advanced Visual Interfaces},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-4131-8},
 editor = {Buono, Paolo and Lanzilotti, Rosa and Matera, Maristella}
}


@online{Cabello.2019,
 abstract = {JavaScript 3D library. Contribute to mrdoob/three.js development by creating an account on GitHub.},
 author = {Cabello, Ricardo},
 year = {2019},
 title = {Three.js: JavaScript 3D library},
 url = {https://github.com/mrdoob/three.js/},
 urldate = {2019-06-17}
}


@book{Campos.2011,
 year = {2011},
 title = {Human-computer interaction - INTERACT 2011: 13th IFIP TC 13 international conference, Lisbon, Portugal, September 5 - 9, 2011 ; proceedings, part II},
 keywords = {Artificial intelligence;Computer science;Education;Information systems;Software engineering},
 address = {Berlin},
 volume = {6947},
 publisher = {Springer},
 isbn = {978-3-642-23770-6},
 series = {Lecture Notes in Computer Science},
 editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
 doi = {10.1007/978-3-642-23771-3}
}


@proceedings{Catarci.2018,
 year = {2018},
 title = {AVI '18: Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-5616-9},
 editor = {Catarci, Tiziana and Leotta, Francesco and Marrella, Andrea and Mecella, Massimo}
}


@incollection{Deller.2011,
 abstract = {Large, public displays are increasingly popular in today's society. For the most part, however, these displays are purely used for information or multimedia presentation, without the possibility of interaction for viewers. On the other hand, personal mobile devices are becoming more and more ubiquitous. Though there are efforts to combine large screens with mobile devices, the approaches are mostly focused on mobiles as control devices, or they are fitted to specific applications. In this paper, we present the ModControl framework, a configurable, modular communication structure that enables large screen applications to connect with personal mobile devices and request a set of configurable modules, utilizing the device as a personalized mobile interface. The main application can easily make use of the highly sophisticated interaction features provided by modern mobile phones. This facilitates new, interactive appealing visualizations that can be actively controlled with an intuitive, unified interface by single or multiple users.},
 author = {Deller, Matthias and Ebert, Achim},
 title = {ModControl -- Mobile Phones as a Versatile Interaction Device for Large Screen Applications},
 keywords = {Distributed interfaces;Input devices and strategies;Interaction framework;User-Centered Design},
 pages = {289--296},
 bookpagination = {page},
 volume = {6947},
 publisher = {Springer},
 isbn = {978-3-642-23770-6},
 series = {Lecture Notes in Computer Science},
 editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
 booktitle = {Human-computer interaction - INTERACT 2011},
 year = {2011},
 address = {Berlin},
 doi = {10.1007/978-3-642-23771-3_22}
}


@online{Denoyel.2016,
 abstract = {Discover how you can explore virtual reality on Sketchfab with our new apps for Oculus, HTC Vive, Gear VR and Cardboard as well as on the web with WebVR.},
 author = {Denoyel, Alban},
 year = {2016},
 title = {Virtual Reality evolved: Sketchfab VR apps and WebVR support},
 url = {https://sketchfab.com/blogs/community/announcing-sketchfab-vr-apps-webvr-support/},
 urldate = {2019-06-23},
 originalyear = {16.05.2016}
}


@online{DevicesandSensorsWorkingGroup.2019,
 abstract = {This specification defines several new DOM events that provide information about the physical orientation and motion of a hosting device.},
 author = {{Devices and Sensors Working Group}},
 editor = {{Devices and Sensors Working Group}},
 year = {2019},
 title = {DeviceOrientation Event Specification: W3C Working Draft, 16 April 2019},
 urldate = {2019-06-17}
}


@online{You.2019,
 abstract = {Vue.js - The Progressive JavaScript Framework},
 author = {You, Evans},
 year = {2019},
 title = {Vue.js},
 url = {https://vuejs.org/},
 urldate = {2019-06-17}
}


@inproceedings{Dias.2018,
 abstract = {Gamepads and 3D controllers are the main controllers used in most Virtual Environments. Despite being simple to use, these input devices have a number of limitations as fixed layout and difficulty to remember the mapping between buttons and functions. Mobile devices present interesting characteristics that might be valuable in immersive environments: more flexible interfaces, touchscreen combined with onboard sensors that allow new interaction and easy acceptance since these devices are used daily by most users. The work described in this article proposes a solution that uses mobile devices to interact with Immersive Virtual Environments for selection and navigation tasks. The proposed solution uses the mobile device camera to track the Head-Mounted-Display position and present a virtual representation of the mobile device screen; it was tested using an Immersive Virtual Museum as use case. Based on this prototype, a study was performed to compare controller based and mobile based interaction for navigation and selection showing that using mobile devices is viable in this context and offers interesting interaction opportunities.},
 author = {Dias, Paulo and Afonso, Luis and Eliseu, S{\'e}rgio and Santos, Beatriz Sousa},
 title = {Mobile devices for interaction in immersive virtual environments},
 url = {http://dl.acm.org.eaccess.ub.tum.de/ft_gateway.cfm?id=3206526&type=pdf},
 keywords = {3D interaction;immersive virtual reality;mobile devices},
 urldate = {2019-06-24},
 publisher = {ACM},
 isbn = {978-1-4503-5616-9},
 editor = {Catarci, Tiziana and Leotta, Francesco and Marrella, Andrea and Mecella, Massimo},
 booktitle = {AVI '18: Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
 year = {2018},
 address = {New York, NY, USA},
 doi = {10.1145/3206505.3206526}
}


@book{ECMAInternational.2018,
 abstract = {This Ecma Standard defines the ECMAScript 2018 Language. It is the ninth edition of the ECMAScript Language Specification. Since publication of the first edition in 1997, ECMAScript has grown to be one of the world's most widely used general-purpose programming languages. It is best known as the language embedded in web browsers but has also been widely adopted for server and embedded applications.},
 author = {{ECMA International}},
 year = {2018},
 title = {Standard ECMA-262: ECMAScript 2018 Language Specification},
 url = {https://www.ecma-international.org/publications/standards/Ecma-262.htm},
 keywords = {javascript},
 urldate = {2019-06-24},
 edition = {9},
 originalyear = {1999}
}


@book{Evans.1999,
 abstract = {We have developed a software tool, VType, that enables a user wearing virtual reality gloves to enter text while in a virtual world. We present techniques to convert noisy data representing nger movements from the gloves into cleaner signals and determine the nger presses. Since each nger press corresponds to more than one symbol, we then use an algorithm for resolving ambiguity on such overloaded keyboard. We demonstrate that accurate text entry is possible using Vtype.},
 author = {Evans, Francine and Skiena, Steven and Varshney, Amitabh},
 year = {1999},
 title = {VType: Entering Text in a Virtual World},
 keywords = {fine gesture recognition;overloaded keyboards;text entry systems;user interfaces;virtual reality gloves}
}


@article{Finstad.2006,
 abstract = {The System Usability Scale (SUS) was administered verbally to native English and non-native English speakers for several internally deployed applications. It was found that a significant proportion of non-native English speakers failed to understand the word {\textquotedbl}cumbersome{\textquotedbl} in Item 8 of the SUS (that is, {\textquotedbl}I found the system to be very cumbersome to use.{\textquotedbl}) This finding has implications for reliability and validity when the questionnaire is distributed electronically in multinational usability efforts.},
 author = {Finstad, Kraig},
 year = {2006},
 title = {The System Usability Scale and Non-native English Speakers},
 url = {http://dl.acm.org.eaccess.ub.tum.de/citation.cfm?id=2835531.2835535},
 keywords = {international usability;language;multicultural;multinational;questionnaire;survey;SUS;system usability scale;usability findings;usability methods},
 pages = {185--188},
 pagination = {page},
 volume = {1},
 number = {4},
 issn = {1931-3357},
 journal = {J. Usability Studies}
}


@inproceedings{Frees.2006,
 abstract = {We present a novel text input interface for immersive virtual environments called CTD, or {\textquotedbl}Connect the Dots{\textquotedbl}. The CTD interface is a small virtual panel containing a grid of dots the user can connect to form alphanumeric characters using a hand held stylus. Coupled with a physical paddle or desk for force feedback, the CTD provides an intuitive text input interface similar to using a pen and paper.},
 author = {Frees, S. and Khouri, R. and Kessler, G. D.},
 title = {Connecting the Dots: Simple Text Input in Immersive Environments},
 keywords = {text input},
 pages = {265--268},
 bookpagination = {page},
 publisher = {{IEEE Operations Center}},
 isbn = {1-4244-0224-7},
 booktitle = {Virtual Reality Conference, 2006},
 year = {2006},
 address = {Piscataway, NJ},
 doi = {10.1109/VR.2006.36}
}


@incollection{Gonzalez.2009,
 abstract = {This chapter describes six different text input techniques and their evaluation. Two techniques stand out from the others. First, a mobile phone keyboard turn out to be a valuable option, offering high typing speed and low typing errors. Second, handwritten character recognition did not perform as good as expected. All findings from this and previous experiments are collected in a proposed guidance tool which will promote their application in future projects.},
 author = {Gonz{\'a}lez, Gabriel and Molina, Jos{\'e} P. and Garc{\'i}a, Arturo S. and Mart{\'i}nez, Diego and Gonz{\'a}lez, Pascual},
 title = {Evaluation of Text Input Techniques in Immersive Virtual Environments},
 pages = {109--118},
 bookpagination = {page},
 publisher = {{Springer-Verlag London}},
 isbn = {978-1-84882-351-8},
 editor = {Latorre, Pedro M. and {Granollers Saltiveri}, Antoni and Mac{\'i}as, Jos{\'e} A.},
 booktitle = {New Trends on Human-Computer Interaction},
 year = {2009},
 address = {London},
 doi = {10.1007/978-1-84882-352-5_11}
}


@online{GoogleLLC.2019,
 abstract = {Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.},
 author = {{Google LLC}},
 year = {2019},
 title = {Protocol Buffers},
 url = {https://developers.google.com/protocol-buffers/},
 urldate = {2019-06-19}
}


@online{GoogleLLC.2019b,
 abstract = {When you are finished with your 3D creation, you can save your Tilt Brush sketch to your computer, or share it to Poly. You can also take snapshot, GIFs or videos of your art to send to friends or post online.},
 author = {{Google LLC}},
 year = {2019},
 title = {Tilt Brush Help: Saving and sharing your Tilt Brush sketches},
 url = {https://support.google.com/tiltbrush/answer/6389651?hl=en},
 urldate = {2019-06-26}
}


@inproceedings{Graf.2012,
 abstract = {This paper presents a concept for the use of a smartphone as a handheld input device for the interaction with 3D visualizations and presentations. Applications are mainly in the area of exhibitions and museums to enable visitors to interact with certain exhibits. Moreover applications for business presentations or computer games are possible. The usability of motion and position sensors in modern smartphones for the purpose of 3D navigation is examined. An algorithm to prevent from position drifting on short distances is developed. Finally a demo application presents the navigation concept and the ability for multiple users to simultaneously interact with the same visualization.},
 author = {Graf, Henning and Jung, Klaus},
 title = {The smartphone as a 3D input device},
 keywords = {3D navigation;human-device interaction;mobile devices},
 pages = {254--257},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-1547-0},
 editor = {Hepper, Dietmar},
 booktitle = {2012 IEEE International Conference on Consumer Electronics - Berlin (ICCE-Berlin 2012)},
 year = {2012},
 address = {Piscataway, NJ},
 doi = {10.1109/ICCE-Berlin.2012.6336487}
}


@inproceedings{Grandi.2016,
 abstract = {We present a 3D user interface for collaborative manipulation ofthree-dimensional objects in virtual environments. It maps inertial sensors, touch screen and physical buttons of a mobile phone into well-known gestures to alter the position, rotation and scale of virtualobjects. As these transformations require the control of multiple degrees of freedom (DOFs), collaboration is proposed as a solution to coordinate the modification of each and all the available DOFs. Users are free to decide their own manipulation roles. All virtual elements are displayed in a single shared screen, which is handy to aggregate multiple users in the same physical space.},
 author = {Grandi, Jeronimo G. and Berndt, Iago and Debarba, Henrique G. and Nedel, Luciana and Maciel, Anderson},
 title = {Collaborative 3D manipulation using mobile phones},
 keywords = {3D interaction;Collaboration;controller;mobile devices},
 pages = {279--280},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1},
 editor = {Thomas, Bruce H. and Lindeman, Rob and Marchal, Maud},
 booktitle = {2016 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2016},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2016.7460079}
}


@proceedings{Hachet.2010,
 year = {2010},
 title = {IEEE Symposium on 3D User Interfaces (3DUI), 2010 ; Waltham, Massachusetts, USA, 20 - 21 March 2010},
 keywords = {Benutzeroberfl{\"a}che;Congresses;Dreidimensionales Bild;Erweiterte Realit{\"a}t;Three-dimensional display systems;User interfaces (Computer systems);Virtual reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4244-6846-1},
 editor = {Hachet, Martin},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Symposium on 3D User Interfaces} and 3DUI}
}


@proceedings{Hepper.2012,
 year = {2012},
 title = {2012 IEEE International Conference on Consumer Electronics - Berlin (ICCE-Berlin 2012): Berlin, Germany, 3 - 5 September 2012},
 keywords = {ilmpub},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-1547-0},
 editor = {Hepper, Dietmar},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Consumer Electronics Society} and {IEEE International Conference on Consumer Electronics - Berlin} and {IEEE ICCE-Berlin}}
}


@proceedings{HumanFactors.2002,
 year = {2002},
 title = {Proceedings of the Human Factors and Ergonomics Society: 46th Annual Meeting, Baltimore, Maryland, September 30 - October 4, 2002 : Bridging Fundamentals {\&} New Opportunities},
 url = {https://books.google.de/books?id=GuZfnQAACAAJ},
 address = {Santa Monica, Calif.},
 publisher = {{SAGE Publications}},
 editor = {{Human Factors} and {Ergonomics Society. Annual meeting}},
 institution = {{The Society}}
}


@proceedings{IEEESymposiumon3DUserInterfaces.2017,
 year = {2017},
 title = {2017 IEEE Symposium on 3D User Interfaces (3DUI): Proceedings : March 18-19, 2017, Los Angeles, CA, USA},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 institution = {{IEEE Symposium on 3D User Interfaces} and {IEEE Computer Society} and 3DUI}
}


@proceedings{Jacko.2007,
 year = {2007},
 title = {Human-Computer Interaction. Interaction Platforms and Techniques: 12th International Conference, HCI International 2007, Beijing, China, July 22-27, 2007, Proceedings, Part II},
 publisher = {{Springer, Berlin, Heidelberg}},
 isbn = {978-3-540-73107-8},
 editor = {Jacko, Julie A.}
}


@book{ECMAInternational.2017,
 abstract = {JSON is a lightweight, text-based, language-independent syntax for defining data interchange formats. It was derived from the ECMAScript programming language, but is programming language independent. JSON defines a small set of structuring rules for the portable representation of structured data.},
 author = {{ECMA International}},
 year = {2017},
 title = {Standard ECMA-404: The JSON Data Interchange Syntax},
 url = {https://ecma-international.org/publications/standards/Ecma-404.htm},
 keywords = {javascript;JSON},
 urldate = {2019-06-24},
 edition = {2},
 originalyear = {2013}
}


@article{Zhang.2015,
 abstract = {Increasing sources of sensor measurements and prior knowledge have become available for indoor localization on smartphones. How to effectively utilize these sources for enhancing localization accuracy is an important yet challenging problem. In this paper, we present an area state-aided localization algorithm that exploits various sources of information. Specifically, we introduce the concept of area state to indicate the area where the user is on an indoor map. The position of the user is then estimated using inertial measurement unit (IMU) measurements with the aid of area states. The area states are in turn updated based on the position estimates. To avoid accumulated errors of IMU measurements, our algorithm uses WiFi received signal strength indicator (RSSI) to indicate the vicinity of the user to the routers. The experiment results show that our system can achieve satisfactory localization accuracy in a typical indoor environment.},
 author = {Zhang, Kaiqing and Hu, Hong and Dai, Wenhan and Shen, Yuan and Win, Moe Z.},
 year = {2015},
 title = {Indoor Localization Algorithm For Smartphones},
 volume = {abs/1503.07628},
 journal = {CoRR}
}


