% This file was created with Citavi 6.3.0.0

@inproceedings{Afonso.2017,
 abstract = {How does the virtual representation of the user's hands influence the performance on a button selection task performed in a tabletbased interaction within an immersive virtual environment? To answer this question, we asked 55 participants to use three conditions: no-hand avatar, realistic avatar and translucent avatar. The participants were faster but made slightly more errors while using the no-avatar condition, and considered easier to perform the task with the translucent avatar.},
 author = {Afonso, Luis and Dias, Paulo and Ferreira, Carlos and Santos, Beatriz Sousa},
 title = {Effect of hand-avatar in a selection task using a tablet as input device in an immersive virtual environment},
 keywords = {button selection task;hand-avatar;immersive virtual environment;input device;mobile device;User study},
 pages = {247--248},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 booktitle = {2017 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2017.7893364}
}


@proceedings{IEEESymposiumon3DUserInterfaces.2017,
 year = {2017},
 title = {2017 IEEE Symposium on 3D User Interfaces (3DUI): Proceedings : March 18-19, 2017, Los Angeles, CA, USA},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-6716-9},
 institution = {{IEEE Symposium on 3D User Interfaces} and {IEEE Computer Society} and 3DUI}
}


@incollection{Jota.2010,
 abstract = {Ray-pointing  techniques  are  often  advocated  as  a  way  for  people  to interact with very large displays from several meters away. We are  interested  in  two  factors  that  can  affect  ray  pointing:  the  par-ticular technique's control type, and parallax.  Consequently,  we  tested  four  ray  pointing  variants  on  a  wall  display  that  covers  a  large  part  of  the  user's  field  of  view.  Tasks  included horizontal and vertical targeting, and tracing. Our results show  that  (a)  techniques  based  on  'rotational  control'  perform  better for targeting tasks, and (b) techniques with low parallax are best  for  tracing  tasks.  We  also  show  that  a  Fitts's  law  analysis  based  on  angles  (as  opposed  to  linear  distances)  better  approx-imates people's ray pointing performance.},
 author = {Jota, Ricardo and Nacenta, Miguel A. and Jorge, Joaquim A. and Carpendale, Sheelagh and Greenberg, Saul},
 title = {A comparison of ray pointing techniques for very large displays},
 url = {http://dl.acm.org.eaccess.ub.tum.de/ft_gateway.cfm?id=1839261&type=pdf},
 keywords = {distant pointing;image-plane;index of difficulty;ISO 9241;Large displays;parallax;ray pointing;targeting;tracing},
 urldate = {2019-06-20},
 pages = {269--276},
 bookpagination = {page},
 publisher = {{Canadian Information Processing Society (CIPS)}},
 isbn = {978-1-56881-712-5},
 editor = {Mould, David and No{\"e}l, Sylvie},
 booktitle = {Graphics interface 2010},
 year = {2010},
 address = {Mississauga, Ontario}
}


@phdthesis{Kamm.2018,
 abstract = {One of the primary goals in the mixed reality continuum is it to create interaction techniques that feel as natural and immersive as possible. However, it is unclear to which degree this naturalism can be achieved and whether it is always a desirable goal.In the past, many selection techniques have prioritized high accuracy and usability but on modern hardware devices, like the leap motion controller, more natural and bare-handbased interactions have been designed. What is still lacking is a test of these techniques outside of desktop-based environments.With this thesis, we contribute to the discussion around new selection methods in three ways. (1) We introduce the HeadRay, a relative pointing technique for VR, which does not depend on the precise position of the hand, and which therefore is very well suited for use with gesture-based input devices like the Myo armband. We show in a user survey that our technique is a well-working selection method that substantially increases immersion. At the same time, it still suffers from inconsistency in the gesture recognition of the Myo that hamper the usability. (2) We show that it is possible to design test scenarios that fully utilize the virtual space around the user and still allow to make measurements that highly correlate with objective performance and subjective ratings by the user. (3) We demonstrate that the Depth Marker proposed by Grossman et al. is a good way to handle occlusion in VR.},
 author = {Kamm, Clemens},
 year = {2018},
 title = {Precision of Pointing with Myo: A Comparison of Controller- and Gesture-Based Selection in Virtual Reality},
 url = {https://drive.google.com/file/d/10LiDxn3zx_S8gCAcJN7eC8iK1a8aKflf/view},
 address = {Munich},
 urldate = {2019-06-20},
 school = {{Technische Universit{\"a}t M{\"u}nchen}},
 type = {Master's Thesis}
}


@inproceedings{Katsuragawa.2016,
 abstract = {We describe the design and evaluation of a freehand, smartwatchbased, mid-air pointing and clicking interaction technique, called Watchpoint. Watchpoint enables a user to point at a target on a nearby large display by moving their arm. It also enables target selection through a wrist rotation gesture. We validate the use of Watchpoint by comparing its performance with two existing techniques: Myopoint, which uses a specialized forearm mounted motion sensor, and a camera-based (Vicon) motion capture system. We show that Watchpoint is statistically comparable in speed and error rate to both systems and, in fact, outperforms in terms of error rate for small (high Fitts's ID) targets. Our work demonstrates that a commodity smartwatch can serve as an effective pointing device in ubiquitous display environments.},
 author = {Katsuragawa, Keiko and Pietroszek, Krzysztof and Wallace, James R. and Lank, Edward},
 title = {Watchpoint: Freehand Pointing with a Smartwatch in a Ubiquitous Display Environment},
 keywords = {Large displays;pointing;smartwatch;wearable},
 pages = {128--135},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {9781450341318},
 editor = {Costabile, Maria Francesca and Buono, Paolo and Matera, Maristella and Lanzilotti, Rosa},
 booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
 year = {2016},
 address = {New York, NY},
 doi = {10.1145/2909132.2909263}
}


@inproceedings{Katzakis.2010,
 abstract = {Conventional input devices such as the mouse and keyboard lack in intuitiveness when it comes to 3D manipulation tasks. In this paper, we explore the use of accelerometer and magnetometer equipped mobile phones as 3-DOF controllers in a 3D rotation task. We put the mobile phone up against the established standards, a mouse and a touch pen and compare their performance. Our preliminary evaluation indicates that for this type of task, with only 5 minutes of practice the mobile device is significantly faster than both the mouse and the touch pen.},
 author = {Katzakis, Nicholas and Hori, Masahiro},
 title = {Mobile devices as multi-DOF controllers},
 keywords = {3D input;accelerometer;controller;interaction;magnetometer;rotation;touchscreen},
 pages = {139--140},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4244-6846-1},
 editor = {Hachet, Martin},
 booktitle = {IEEE Symposium on 3D User Interfaces (3DUI), 2010 ; Waltham, Massachusetts, USA, 20 - 21 March 2010},
 year = {2010},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2010.5444700}
}


@phdthesis{Koetsier.2016,
 abstract = {Vampires is a framework that assists in finding the optimal combination of resources to use for execution of a set of independent tasks in a heterogeneous cloud environment. This thesis discusses the decisions made during the development of a web-based user interface for Vampires. This includes a thorough evaluation of client-side JavaScript frameworks, resulting in the choice of AngularJS to use as the basis of the Vampires user interface.},
 author = {Koetsier, Jaap},
 year = {2016},
 title = {Evaluation of JavaScript frame-works for the development of a web-based user interface for Vampires}
}


@proceedings{Lecuyer.2013,
 year = {2013},
 title = {2013 IEEE Symposium on 3D User Interfaces (3DUI): 16 - 17 March 2013, Orlando, Florida, USA},
 keywords = {Benutzeroberfl{\"a}che;Congresses;Dreidimensionales Bild;Erweiterte Realit{\"a}t;Three-dimensional display systems;User interfaces (Computer systems);Virtual reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6098-2},
 editor = {L{\'e}cuyer, Anatole},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Symposium on 3D User Interfaces} and 3DUI}
}


@proceedings{Hepper.2012,
 year = {2012},
 title = {2012 IEEE International Conference on Consumer Electronics - Berlin (ICCE-Berlin 2012): Berlin, Germany, 3 - 5 September 2012},
 keywords = {ilmpub},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-1547-0},
 editor = {Hepper, Dietmar},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Consumer Electronics Society} and {IEEE International Conference on Consumer Electronics - Berlin} and {IEEE ICCE-Berlin}}
}


@proceedings{Leong.2014,
 abstract = {Annotation},
 year = {2014},
 title = {Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures the Future of Design},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450306539},
 editor = {Leong, Tuck},
 doi = {10.1145/2686612}
}


@inproceedings{Lipari.2015,
 abstract = {We integrated touch menus into a cohesive smartphone-based VR controller. Smartphone touch surfaces offer new interaction styles and also aid VR interaction when tracking is absent or impreciseor when users have limited arm mobility or fatigue. In Handymenu, a touch surface is split into two areas: one for menu interaction and the other for spatial interactions such as VR object selection, manipulation, navigation, or parameter adjustment. Users in our studies transitioned between the two areas and performed nested, repeated selections. A formal experiment included VR object selection (ray and touch), menu selection (ray and touch), menu layout (pie and grid), as well as touch and visual feedback sizes in some cases (two levels each).},
 author = {Lipari, Nicholas G. and Borst, Christoph W.},
 title = {Handymenu: Integrating menu selection into a multifunction smartphone-based VR controller},
 keywords = {3DTV;Menus;Smartphone;Touch;Virtual reality},
 pages = {129--132},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-6886-5},
 editor = {Lindeman, Rob},
 booktitle = {2015 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2015},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2015.7131737}
}


@book{Mould.2010,
 year = {2010},
 title = {Graphics interface 2010: Conference ; GI 2010] ; Ottawa, Ontario, Canada, 31 May - 2 June 2010 ; proceedings},
 address = {Mississauga, Ontario},
 publisher = {{Canadian Information Processing Society (CIPS)}},
 isbn = {978-1-56881-712-5},
 editor = {Mould, David and No{\"e}l, Sylvie}
}


@inproceedings{Pietroszek.2014,
 abstract = {We introduce and formally evaluate smartcasting: a smartphone-based Ray Casting implementation for 3D environments presented on large, public, autostereoscopic displays. By utilizing a smartphone as an input device, smartcasting enables ``walk up and use'' interaction with large displays, without the need for expensive tracking systems or specialized pointing devices. Through an empirical validation we show that the performance and precision of smartcasting is comparable to a Wiimotebased raycasting implementation, without requiring specialized hardware or high-precision cameras to enable user interaction.},
 author = {Pietroszek, Krzysztof and Kuzminykh, Anastasia and Wallace, James R. and Lank, Edward},
 title = {Smartcasting: A Discount 3D Interaction Technique  for Public Displays},
 keywords = {3D displays;3D environnent;3D interaction;interaction technique;mobile interaction;raycasting},
 pages = {119--128},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {978-1-4503-0653-9},
 editor = {{Tuck Wah Leong}},
 booktitle = {Proceedings of the 26th Australian Computer-Human Interaction Conference  on Designing Futures - the Future of Design, OZCHI '14, Sydney,  New South Wales, Australia, December 2-5, 2014},
 year = {2014},
 doi = {10.1145/2686612.2686629}
}


@inproceedings{Steed.2013,
 abstract = {With the increasing power of mobile CPUs and GPUs, it is becoming tractable to integrate all the components of an interactive, immersive virtual reality system onto a small mobile device. We present a demonstration of a head-mounted display system integrated onto an iPhone-based platform. In building this demonstration we tackled two main problems. First, how to integrate the userinterface, utilizing the phone itself as an unseen touch interface. Second, how to integrate multiple inertial measuring units to facilitate user interaction. The resulting system indicates the practicality of mobile virtual reality systems based on smartphones.},
 author = {Steed, Anthony and Julier, Simon},
 title = {Design and implementation of an immersive virtual reality system based on a smartphone platform},
 keywords = {3D user interaction;head-mounted display;Mobile virtual reality;selection tasks},
 pages = {43--46},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-6098-2},
 editor = {L{\'e}cuyer, Anatole},
 booktitle = {2013 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2013},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2013.6550195}
}


@proceedings{Tan.2011,
 year = {2011},
 title = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450302685},
 series = {ACM Digital Library},
 editor = {Tan, Desney},
 institution = {{Association for Computing Machinery}},
 doi = {10.1145/1979742}
}


@proceedings{Thomas.2016,
 year = {2016},
 title = {2016 IEEE Symposium on 3D User Interfaces (3DUI): Greenville, South Carolina, USA, 19-20 March 2016 : proceedings},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1},
 editor = {Thomas, Bruce H. and Lindeman, Rob and Marchal, Maud},
 institution = {{IEEE Symposium on 3D User Interfaces} and {Institute of Electrical and Electronics Engineers} and 3DUI and {IEEE Virtual Reality} and {IEEE VR}}
}


@proceedings{TuckWahLeong.2014,
 year = {2014},
 title = {Proceedings of the 26th Australian Computer-Human Interaction Conference  on Designing Futures - the Future of Design, OZCHI '14, Sydney,  New South Wales, Australia, December 2-5, 2014},
 url = {https://doi.org/10.1145/2686612,  [Add to Citavi project by DOI]},
 publisher = {ACM},
 isbn = {978-1-4503-0653-9},
 editor = {{Tuck Wah Leong}},
 doi = {10.1145/2686612}
}


@proceedings{Lindeman.2015,
 year = {2015},
 title = {2015 IEEE Symposium on 3D User Interfaces (3DUI): 23 - 24 March 2015, Arles, France},
 keywords = {Benutzeroberfl{\"a}che;Dreidimensionales Bild;Erweiterte Realit{\"a}t},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4673-6886-5},
 editor = {Lindeman, Rob},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society} and {IEEE Symposium on 3D User Interfaces} and 3DUI and {IEEE 3DUI Symposium}}
}


@proceedings{UniversitiTeknologiMARA.2011,
 year = {2011},
 title = {International Conference on User Science and Engineering (i-USEr), 2011: Nov. 29 2011 - Dec. 1 2011, Selangor, Malaysia ; proceedings},
 keywords = {Computer software;Congresses;Human factors;Human-computer interaction},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4577-1655-3},
 institution = {{Universiti Teknologi MARA} and {International Conference on User Science and Engineering} and i-USEr}
}


@proceedings{Hachet.2010,
 year = {2010},
 title = {IEEE Symposium on 3D User Interfaces (3DUI), 2010 ; Waltham, Massachusetts, USA, 20 - 21 March 2010},
 keywords = {Benutzeroberfl{\"a}che;Congresses;Dreidimensionales Bild;Erweiterte Realit{\"a}t;Three-dimensional display systems;User interfaces (Computer systems);Virtual reality},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-4244-6846-1},
 editor = {Hachet, Martin},
 institution = {{Institute of Electrical and Electronics Engineers} and {IEEE Symposium on 3D User Interfaces} and 3DUI}
}


@inproceedings{Graf.2012,
 abstract = {This paper presents a concept for the use of a smartphone as a handheld input device for the interaction with 3D visualizations and presentations. Applications are mainly in the area of exhibitions and museums to enable visitors to interact with certain exhibits. Moreover applications for business presentations or computer games are possible. The usability of motion and position sensors in modern smartphones for the purpose of 3D navigation is examined. An algorithm to prevent from position drifting on short distances is developed. Finally a demo application presents the navigation concept and the ability for multiple users to simultaneously interact with the same visualization.},
 author = {Graf, Henning and Jung, Klaus},
 title = {The smartphone as a 3D input device},
 keywords = {3D navigation;human-device interaction;mobile devices},
 pages = {254--257},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-1547-0},
 editor = {Hepper, Dietmar},
 booktitle = {2012 IEEE International Conference on Consumer Electronics - Berlin (ICCE-Berlin 2012)},
 year = {2012},
 address = {Piscataway, NJ},
 doi = {10.1109/ICCE-Berlin.2012.6336487}
}


@article{Argelaguet.2013,
 abstract = {Computer graphics applications controlled through natural gestures are gaining increasing popularity these days due to recent developments in low-cost tracking systems and gesture recognition technologies. Although interaction techniques through natural gestures have already demonstrated their benefits in manipulation, navigation and avatar-control tasks, effective selection with pointing gestures remains an open problem. In this paper we survey the state-of-the-art in 3D object selection techniques. We review important findings in human control models, analyze major factors influencing selection performance, and classify existing techniques according to a number of criteria. Unlike other components of the application's user interface, pointing techniques need a close coupling with the rendering pipeline, introducing new elements to be drawn, and potentially modifying the object layout and the way the scene is rendered. Conversely, selection performance is affected by rendering issues such as visual feedback, depth perception, and occlusion management. We thus review existing literature paying special attention to those aspects in the boundary between computer graphics and human--computer interaction.},
 author = {Argelaguet, Ferran and Andujar, Carlos},
 year = {2013},
 title = {A survey of 3D object selection techniques for virtual environments},
 pages = {121--136},
 pagination = {page},
 volume = {37},
 number = {3},
 issn = {00978493},
 journal = {Computers {\&} Graphics},
 doi = {10.1016/j.cag.2012.12.003}
}


@article{Ballagas.2006,
 author = {Ballagas, R. and Borchers, J. and Rohs, M. and Sheridan, J. G.},
 year = {2006},
 title = {The Smart Phone: A Ubiquitous Input Device},
 pages = {70--77},
 pagination = {page},
 volume = {5},
 number = {1},
 issn = {1536-1268},
 journal = {IEEE Pervasive Computing},
 doi = {10.1109/MPRV.2006.18}
}


@inproceedings{Bauer.2011,
 abstract = {Due to their size, large high-resolution screens have become popular display devices used in collaborative scenarios. However, traditional interaction methods based on combinations of computer mice and keyboards often do not scale to the number of users or the size of the display. Modern smart phones featuring multi-modal input/output by means of built-in cameras, acceleration sensors, internet capability, touch screens and considerable memory offer a way to address these issues. In the last couple of years they have become common everyday life gadgets. In this paper we conduct an extensive user study comparing the experience of test candidates when using traditional input devices and metaphors with the one when using new smart phone based techniques, like multi-modal drag and tilt. Candidates were asked to complete various 2D and 3D interaction tasks relevant for most applications on a large, monitor-based, high-resolution tiled wall system. Our study evaluates both user performance and satisfaction, identifying strengths and weaknesses of the researched interaction methods in specific tasks. By breaking these tasks down into a well-defined set of subtasks the results of each task are comparable to each other and can be classified by subtask and use case. Results reveal a superior performance of users in certain tasks when using the new interaction techniques. Even first-time users were able to complete a task faster with the smart phone than with traditional devices.},
 author = {Bauer, Jens and Thelen, Sebastian and Ebert, Achim},
 title = {Using smart phones for large-display interaction},
 keywords = {Collaboration;evaluation methods/usability evaluation;interaction with small or large displays;mobility/mobile accessibility/mobile devices;multi-modal interfaces},
 pages = {42--47},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4577-1655-3},
 booktitle = {International Conference on User Science and Engineering (i-USEr), 2011},
 year = {2011},
 address = {Piscataway, NJ},
 doi = {10.1109/iUSEr.2011.6150533}
}


@inproceedings{Benzina.2011,
 abstract = {We introduce a one-handed travel technique for virtualenvironments (VE), we call Phone-Based MotionControl. The travel technique uses a mobile phone withintegrated sensors as a 3D spatial input device. Webenefit from the touch capability to change theviewpoint translation in the VE, while the orientation ofthe viewpoint in the VE is controlled by the built-insensors. The travel interaction clearly distinguishesbetween translation (touch based translation control)and rotation (steer based rotation control), puttingeach set of degrees of freedom to a separateinteraction technique.This work examines how many degrees of freedom areneeded to perform the travel task as easy as possible.It also investigates different mapping functionsbetween the user's actions and the viewpoint reactionsin the VR. For that purpose, four metaphors aredeveloped for the steer based rotation controltechnique. The results of the user study indicate thetrend that 4 DOF metaphors perform best, and that theusage of a mobile roll to control the viewpoint is thedesired mapping.},
 author = {Benzina, Amal and Toennis, Marcus and Klinker, Gudrun and Ashry, Mohamed},
 title = {Phone-based motion control in VR},
 keywords = {degree of freedom;interaction;Navigation;Travel;User study},
 pages = {1519},
 bookpagination = {page},
 publisher = {ACM},
 isbn = {9781450302685},
 series = {ACM Digital Library},
 editor = {Tan, Desney},
 booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 year = {2011},
 address = {New York, NY},
 doi = {10.1145/1979742.1979801}
}


@incollection{Berge.,
 abstract = {3D Virtual Environments (3DVE) come up as a good solution to transmit knowledge in a museum exhibit. In such contexts, providing easy to learn and to use interaction techniques which facilitate the handling inside a 3DVE is crucial to maximize the knowledge transfer. We took the opportunity to design and implement a software platform for explaining the behavior of the Telescope Bernard-Lyot to museum visitors on top of the Pic du Midi. Beyond the popularization of a complex scientific equipment, this platform constitutes an open software environment to easily plug different 3D interaction techniques. Recently, popular use of a smartphones as personal handled computer lets us envision the use of a mobile device as an interaction support with these 3DVE. Accordingly, we design and propose how to use the smartphone as a tangible object to navigate inside a 3DVE. In order to prove the interest in the use of smartphones, we compare our solution with available solutions: keyboard-mouse and 3D mouse. User experiments confirmed our hypothesis and particularly emphasizes that visitors find our solution more attractive and stimulating. Finally, we illustrate the benefits of our software framework by plugging alternative interaction techniques for supporting selection and manipulation task in 3D.},
 author = {Berg{\'e}, Louis-Pierre and Perelman, Gary and Hamelin, Adrien and Raynal, Mathieu and Sanza, C{\'e}dric and Houry-Panchetti, Minica and Cabanac, R{\'e}mi and Dubois, Emmanuel},
 title = {Smartphone Based 3D Navigation Techniques in an Astronomical Observatory Context: Implementation and Evaluation in a Software Platform},
 url = {https://hal.archives-ouvertes.fr/hal-01387801/document},
 keywords = {3D environment;3D navigation;Experiment;Interaction with smartphone;Interactive visualization;Museum exhibit;Software platform}
}


@proceedings{Buono.2016,
 year = {2016},
 title = {AVI '16: Proceedings of the International Working Conference on Advanced Visual Interfaces},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-4131-8},
 editor = {Buono, Paolo and Lanzilotti, Rosa and Matera, Maristella}
}


@online{Cabello.2019,
 abstract = {JavaScript 3D library. Contribute to mrdoob/three.js development by creating an account on GitHub.},
 author = {Cabello, Ricardo},
 year = {2019},
 title = {Three.js: JavaScript 3D library},
 url = {https://github.com/mrdoob/three.js/},
 urldate = {2019-06-17}
}


@inproceedings{Grandi.2016,
 abstract = {We present a 3D user interface for collaborative manipulation ofthree-dimensional objects in virtual environments. It maps inertial sensors, touch screen and physical buttons of a mobile phone into well-known gestures to alter the position, rotation and scale of virtualobjects. As these transformations require the control of multiple degrees of freedom (DOFs), collaboration is proposed as a solution to coordinate the modification of each and all the available DOFs. Users are free to decide their own manipulation roles. All virtual elements are displayed in a single shared screen, which is handy to aggregate multiple users in the same physical space.},
 author = {Grandi, Jeronimo G. and Berndt, Iago and Debarba, Henrique G. and Nedel, Luciana and Maciel, Anderson},
 title = {Collaborative 3D manipulation using mobile phones},
 keywords = {3D interaction;Collaboration;controller;mobile devices},
 pages = {279--280},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5090-0842-1},
 editor = {Thomas, Bruce H. and Lindeman, Rob and Marchal, Maud},
 booktitle = {2016 IEEE Symposium on 3D User Interfaces (3DUI)},
 year = {2016},
 address = {Piscataway, NJ},
 doi = {10.1109/3DUI.2016.7460079}
}


@book{Campos.2011,
 year = {2011},
 title = {Human-computer interaction - INTERACT 2011: 13th IFIP TC 13 international conference, Lisbon, Portugal, September 5 - 9, 2011 ; proceedings, part II},
 keywords = {Artificial intelligence;Computer science;Education;Information systems;Software engineering},
 address = {Berlin},
 volume = {6947},
 publisher = {Springer},
 isbn = {978-3-642-23770-6},
 series = {Lecture Notes in Computer Science},
 editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
 doi = {10.1007/978-3-642-23771-3}
}


@incollection{Deller.2011,
 abstract = {Large, public displays are increasingly popular in today's society. For the most part, however, these displays are purely used for information or multimedia presentation, without the possibility of interaction for viewers. On the other hand, personal mobile devices are becoming more and more ubiquitous. Though there are efforts to combine large screens with mobile devices, the approaches are mostly focused on mobiles as control devices, or they are fitted to specific applications. In this paper, we present the ModControl framework, a configurable, modular communication structure that enables large screen applications to connect with personal mobile devices and request a set of configurable modules, utilizing the device as a personalized mobile interface. The main application can easily make use of the highly sophisticated interaction features provided by modern mobile phones. This facilitates new, interactive appealing visualizations that can be actively controlled with an intuitive, unified interface by single or multiple users.},
 author = {Deller, Matthias and Ebert, Achim},
 title = {ModControl -- Mobile Phones as a Versatile Interaction Device for Large Screen Applications},
 keywords = {Distributed interfaces;Input devices and strategies;Interaction framework;User-Centered Design},
 pages = {289--296},
 bookpagination = {page},
 volume = {6947},
 publisher = {Springer},
 isbn = {978-3-642-23770-6},
 series = {Lecture Notes in Computer Science},
 editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
 booktitle = {Human-computer interaction - INTERACT 2011},
 year = {2011},
 address = {Berlin},
 doi = {10.1007/978-3-642-23771-3_22}
}


@online{Denoyel.2016,
 abstract = {Discover how you can explore virtual reality on Sketchfab with our new apps for Oculus, HTC Vive, Gear VR and Cardboard as well as on the web with WebVR.},
 author = {Denoyel, Alban},
 year = {2016},
 title = {Virtual Reality evolved: Sketchfab VR apps and WebVR support: New on Sketchfab - 16 May 2016},
 url = {https://sketchfab.com/blogs/community/announcing-sketchfab-vr-apps-webvr-support/},
 urldate = {2019-06-23},
 originalyear = {16.05.2016}
}


@online{DevicesandSensorsWorkingGroup.2019,
 abstract = {This specification defines several new DOM events that provide information about the physical orientation and motion of a hosting device.},
 author = {{Devices and Sensors Working Group}},
 editor = {{Devices and Sensors Working Group}},
 year = {2019},
 title = {DeviceOrientation Event Specification: Editor's Draft, 15 April 2019},
 url = {https://w3c.github.io/deviceorientation/#dom-deviceorientationevent-alpha},
 urldate = {2019-06-17}
}


@incollection{Dias.,
 abstract = {Gamepads and 3D controllers are the main controllers used in most Virtual Environments. Despite being simple to use, these input devices have a number of limitations as fixed layout and difficulty to remember the mapping between buttons and functions. Mobile devices present interesting characteristics that might be valuable in immersive environments: more flexible interfaces, touchscreen combined with onboard sensors that allow new interaction and easy acceptance since these devices are used daily by most users. The work described in this article proposes a solution that uses mobile devices to interact with Immersive Virtual Environments for selection and navigation tasks. The proposed solution uses the mobile device camera to track the Head-Mounted-Display position and present a virtual representation of the mobile device screen; it was tested using an Immersive Virtual Museum as use case. Based on this prototype, a study was performed to compare controller based and mobile based interaction for navigation and selection showing that using mobile devices is viable in this context and offers interesting interaction opportunities.},
 author = {Dias, Paulo and Afonso, Luis and Eliseu, S{\'e}rgio and Santos, Beatriz Sousa},
 title = {Mobile devices for interaction in immersive virtual environments},
 url = {http://dl.acm.org.eaccess.ub.tum.de/ft_gateway.cfm?id=3206526&type=pdf},
 keywords = {3D interaction;immersive virtual reality;mobile devices},
 doi = {10.1145/3206505.3206526}
}


@book{ECMAInternational.2017,
 abstract = {JSON is a lightweight, text-based, language-independent syntax for defining data interchange formats. It was derived from the ECMAScript programming language, but is programming language independent. JSON defines a small set of structuring rules for the portable representation of structured data.},
 author = {{ECMA International}},
 year = {2017},
 title = {Standard ECMA-404: The JSON Data Interchange Syntax},
 url = {https://ecma-international.org/publications/standards/Ecma-404.htm},
 keywords = {javascript;JSON},
 urldate = {2019-06-24},
 edition = {2},
 originalyear = {2013}
}


@book{ECMAInternational.2018,
 abstract = {This Ecma Standard defines the ECMAScript 2018 Language. It is the ninth edition of the ECMAScript Language Specification. Since publication of the first edition in 1997, ECMAScript has grown to be one of the world's most widely used general-purpose programming languages. It is best known as the language embedded in web browsers but has also been widely adopted for server and embedded applications.},
 author = {{ECMA International}},
 year = {2018},
 title = {Standard ECMA-262: ECMAScript 2018 Language Specification},
 url = {https://www.ecma-international.org/publications/standards/Ecma-262.htm},
 keywords = {javascript},
 urldate = {2019-06-24},
 edition = {9},
 originalyear = {1999}
}


@online{GoogleLLC.2019,
 abstract = {Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.},
 author = {{Google LLC}},
 year = {2019},
 title = {Protocol Buffers ~|~ Google Developers},
 url = {https://developers.google.com/protocol-buffers/},
 urldate = {2019-06-19}
}


@proceedings{Costabile.2016,
 year = {2016},
 title = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
 keywords = {Computer science},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450341318},
 editor = {Costabile, Maria Francesca and Buono, Paolo and Matera, Maristella and Lanzilotti, Rosa},
 doi = {10.1145/2909132}
}


@online{You.2019,
 abstract = {Vue.js - The Progressive JavaScript Framework},
 author = {You, Evans},
 year = {2019},
 title = {Vue.js},
 url = {https://vuejs.org/},
 urldate = {2019-06-17}
}


